{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- find an example with the same data [here](https://ch.mathworks.com/help/signal/ug/classify-ecg-signals-using-long-short-term-memory-networks.html;jsessionid=c17fc0aaa4ca70151c8705f668c5)!\n",
    "- Hz = cycles per second.\n",
    "- class imbalance: 3030, 443, 1474, 170\n",
    "- classes are: \n",
    "> ... normal sinus rhythm, atrial fibrillation (AF), an alternative rhythm, or is too noisy to be classified. - [link](https://physionet.org/content/challenge-2017/1.0.0/)\n",
    "\n",
    "> Time-frequency moments provide an efficient way to characterize signals whose frequencies change in time (that is, are nonstationary). Such signals can arise from machinery with degraded or failed hardware. Classical Fourier analysis cannot capture the time-varying frequency behavior. Time-frequency distribution generated by short-time Fourier transform (STFT) or other time-frequency analysis techniques can capture the time-varying behavior, but directly treating these distributions as features carries a high computational burden, and potentially introduces unrelated and undesirable feature characteristics. In contrast, distilling the time-frequency distribution results into low-dimension time-frequency moments provides a method for capturing the essential features of the signal in a much smaller data package. Using these moments significantly reduces the computational burden for feature extraction and comparison — a key benefit for real-time operation [1], [2]. - [link](https://ww2.mathworks.cn/help/predmaint/ref/tfsmoment.html)\n",
    "\n",
    "> Training the LSTM network using raw signal data results in a poor classification accuracy. Training the network using two time-frequency-moment features for each signal significantly improves the classification performance and also decreases the training time. - [link](https://ch.mathworks.com/help/signal/ug/classify-ecg-signals-using-long-short-term-memory-networks.html;jsessionid=c17fc0aaa4ca70151c8705f668c5)\n",
    "\n",
    "> time-order of the columns is ignored by tabular learning algorithms (but used by time series classification and regression algorithms) - [link](https://towardsdatascience.com/sktime-a-unified-python-library-for-time-series-machine-learning-3c103c139a55)\n",
    "\n",
    "> The estimators in sktime extend scikit-learn’s regressors and classifiers to their time series counterparts. Sktime also includes new estimators specific to time series tasks. - [link](https://towardsdatascience.com/sktime-a-unified-python-library-for-time-series-machine-learning-3c103c139a55)\n",
    "\n",
    "> Shapelet-based classifiers will be better when the best feature might be the presence or absence of a phase-independent pattern in a series. <br>\n",
    "Dictionary-based (BOSS) or frequency-based (RISE) classifiers will be better when you can discriminate using the frequency of a pattern. - [link](https://towardsdatascience.com/a-brief-introduction-to-time-series-classification-algorithms-7b4284d31b97)\n",
    "\n",
    ">  Furthermore, the instantaneous frequency mean might be too high for the LSTM to learn effectively. When a network is fit on data with a large mean and a large range of values, large inputs could slow down the learning and convergence of the network [6]. - [link](https://ch.mathworks.com/help/signal/ug/classify-ecg-signals-using-long-short-term-memory-networks.html;jsessionid=c17fc0aaa4ca70151c8705f668c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR_SnbMArXr7"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "X_train = pd.read_csv('./data/X_train.csv')\n",
    "y_train = pd.read_csv('./data/y_train.csv')\n",
    "\n",
    "X_train['y'] = y_train['y']\n",
    "\n",
    "raw_df = X_train.drop(['id'], axis=1)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of the '\\n' string in the last cell of the features. It was the only string like that. After the removal, all columns are type int64 or float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "# replace string at the end\n",
    "df = df.replace(r'\\\\n', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let't have a look at how the missing data is distributed over our dataframe. We know from that the recordings were sampled at 300 Hz which are 300 cycles per second. Therefor, we need to divide the sample rate by 300 to calculate the time in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = [df[col].isna().sum() for col in df]\n",
    "time = np.arange(0, df.shape[1], 1) / 300\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8,3))\n",
    "ax.plot(time[:-2], na[:-2])\n",
    "ax.set(xlabel='Seconds', ylabel='Nr. of NaNs',\n",
    "       title='NaNs over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how this distribution looks separeted by label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(df['y'], return_counts=True)\n",
    "for i in range(len(labels)):\n",
    "    print('class {}: {}'.format(labels[i], counts[i]))\n",
    "    \n",
    "class_df = dict()\n",
    "na_per_class = dict()\n",
    "for lab in labels:\n",
    "    class_df[lab] = df.loc[df['y'] == lab]\n",
    "    na_per_class[lab] = [class_df[lab][col].isna().sum() for col in class_df[lab]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, figsize=(8,6), constrained_layout=True)\n",
    "for i in range(len(labels)):\n",
    "    ax[0].plot(time[:-2], na_per_class[labels[i]][:-2], label='class {}'.format(labels[i]))\n",
    "    ax[1].plot(time[:-2]*300, (na_per_class[labels[i]][:-2]/counts[i])*100, label='class {}'.format(labels[i]))\n",
    "ax[0].set(xlabel='Seconds', ylabel='Nr. of NaN', title='Absolute NaNs over time')\n",
    "ax[1].set(xlabel='Seconds', ylabel='% NaN', title='% NaNs over time')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is more or less equal in terms of signal length. Class 3 with the fewest samples deviates the most from the general trend.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the missing data in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5), constrained_layout=True)\n",
    "ax.imshow(df.isna().iloc[:, :], aspect='auto')\n",
    "ax.axvline(9000)\n",
    "ax.set(xlabel='Time', ylabel='Samples', title='Signal lengt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note is that the missing data stems only from the recording beeing finished. There is no missing data during the recordings. Now, let's plot a histogram of the signal lengths. We see that most signals have a length of around 8613."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notna_samples = df.notna().sum(axis=1)\n",
    "signal_lengths = np.zeros(len(notna_samples)) + notna_samples\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(7,3), constrained_layout=True)\n",
    "n, bins, _ = ax.hist(signal_lengths[:-1], bins=(len(signal_lengths[:-1])))\n",
    "ax.set(xlabel='Signal lengths', ylabel='Count',\n",
    "       title='Signal lenghts across all samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_count = bins[np.where(n == np.max(n))]\n",
    "print('Most counts at point {:.0f} / {:.2f} seconds'.format(high_count[0], high_count[0]/300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a few examples of the recordings to get a feel for what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(cl, data, no_label=False):\n",
    "    rand = np.random.randint(150, size=5)\n",
    "    fig, ax = plt.subplots(len(rand), figsize=(15,8), constrained_layout=True)\n",
    "    if no_label == True:\n",
    "        for i in range(len(rand)):\n",
    "            data.iloc[rand[i], :].plot(ax=ax[i])\n",
    "            ax[i].set(title='Sample {}'.format(rand[i]))\n",
    "            ax[i].legend(loc='upper left')\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(rand)):\n",
    "            data.loc[df['y'] == cl].iloc[rand[i], :].plot(ax=ax[i], label='class {}'.format(cl))\n",
    "            ax[i].set(title='Sample {}'.format(rand[i]))\n",
    "            ax[i].legend(loc='upper left')\n",
    "        return\n",
    "\n",
    "def plot_specific(signal):\n",
    "    fig, ax = plt.subplots(figsize=(15,3))\n",
    "    signal.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(2, df, False) # choose class or choose to show all classes (chosen class is ignored) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the signals all the same length. We choose 9000, a length just above the main bulk of the most common length. We fill the shorter sequences with zeros and cut the longer sequences to the target length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_padded = df.copy()\n",
    "df_padded = df_padded.fillna(0)\n",
    "\n",
    "t = df_padded.iloc[:, 0:9000]\n",
    "y = df_padded['y']\n",
    "print(t.shape)\n",
    "print(y.shape)\n",
    "\n",
    "plot_examples(1, t, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the padded dati is distributed to check wether we should normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_signal(target_length, dataframe):\n",
    "    notna = dataframe.notna().sum(axis=1)\n",
    "    orig_lengths = np.zeros(len(notna)) + notna\n",
    "    \n",
    "    col_names = list(dataframe.columns)\n",
    "    col_names = col_names[0:target_length]\n",
    "    \n",
    "    collect_samples = list()\n",
    "    for row in range(dataframe.shape[0]):\n",
    "        sample = dataframe.iloc[row, :] # get row as a Series\n",
    "        sample = sample.dropna()\n",
    "        \n",
    "        sample_length = sample.shape[0] # we're handling a Series not a Dataframe\n",
    "        while sample_length < target_length:\n",
    "            duplicated_sample = sample.append(sample)\n",
    "            sample = duplicated_sample\n",
    "            sample_length = sample.shape[0] # we're handling a Series not a Dataframe\n",
    "        \n",
    "        training_sample = sample[0:target_length]\n",
    "        training_sample.index = col_names # index bc. handling a Series\n",
    "        collect_samples.append(training_sample)\n",
    "\n",
    "    finished_df = pd.DataFrame(data=collect_samples)\n",
    "    \n",
    "    return finished_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend = df.copy()\n",
    "\n",
    "y_extend = df_extend.pop('y')\n",
    "t_extend = segment_signal(9000, df_extend)\n",
    "\n",
    "print(t_extend.shape)\n",
    "print(y_extend.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_examples(1, t_extend, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "test_sample = t.iloc[n, :]\n",
    "plot_specific(test_sample)\n",
    "test_sample = pd.Series.to_numpy(test_sample, dtype='float64')\n",
    "sr = 300\n",
    "print(test_sample.shape)\n",
    "print(y[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specs(cl, features, labels, no_label=False):\n",
    "    rand = np.random.randint(150, size=5)\n",
    "    fig, ax = plt.subplots(len(rand), figsize=(15,15), constrained_layout=True)\n",
    "    if no_label == True:\n",
    "        for i in range(len(rand)):\n",
    "            sample = pd.Series.to_numpy(df.iloc[rand[i], :])\n",
    "            D = librosa.amplitude_to_db(np.abs(librosa.stft(sample, n_fft=255, hop_length=1)), ref=np.max)   \n",
    "            D = np.log(D+1e-10)\n",
    "            ax[i].imshow(D, aspect='auto')\n",
    "            ax[i].set(title='Sample {}'.format(rand[i]))\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(rand)):\n",
    "            features['y'] = labels\n",
    "            features = features.loc[df['y'] == cl]\n",
    "            sample = pd.Series.to_numpy(features.iloc[rand[i], :])\n",
    "            D = librosa.amplitude_to_db(np.abs(librosa.stft(sample, n_fft=255, hop_length=1)), ref=np.max)\n",
    "            ax[i].imshow(D, aspect='auto')\n",
    "            ax[i].set(title='Sample {}'.format(rand[i]))\n",
    "        return\n",
    "\n",
    "def plot_specific(signal):\n",
    "    fig, ax = plt.subplots(figsize=(15,3))\n",
    "    signal.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_specs(1, t, y, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample = t_extend.iloc[1, :]\n",
    "plot_specific(test_sample)\n",
    "test_array = pd.Series.to_numpy(test_sample)\n",
    "flat_test = librosa.feature.spectral_flatness(y=test_array, n_fft=50, hop_length=1)[0][0:-1]\n",
    "flat_Series = pd.Series(flat_test)\n",
    "plot_specific(flat_Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D-convolution preprocessing\n",
    "To get a first feel for how the algorithms perform, we don't use cross validation but train and validate the data on single test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import scipy\n",
    "import librosa\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from biosppy.signals import ecg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we  write a function that calculates features from each sample and stacks them in a 3D array together with the raw sample. The array contains \\[samples, time, features\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(array):\n",
    "    \n",
    "    collect = list()\n",
    "    \n",
    "    raw = list()\n",
    "    filtered = list()\n",
    "    flat_test = list()\n",
    "    cent = list()\n",
    "    rolloff_99 = list()\n",
    "    rolloff_05 = list()\n",
    "    m2 = list()\n",
    "    m3 = list()\n",
    "    m4 = list()\n",
    "    \n",
    "    for row in range(array.shape[0]):\n",
    "        \n",
    "        signal = array[row]\n",
    "        raw.append(signal)\n",
    "        #output = ecg.ecg(signal=signal, sampling_rate=300)[1]\n",
    "        #filtered.append(output[1])\n",
    "        #flat_test.append(librosa.feature.spectral_flatness(y=signal, n_fft=50, hop_length=1)[0][0:-1])\n",
    "        #cent.append(librosa.feature.spectral_centroid(y=signal, sr=300, n_fft=255, hop_length=1)[0])\n",
    "        #rolloff_99.append(librosa.feature.spectral_rolloff(y=signal, sr=300, n_fft=255, hop_length=1, roll_percent=0.99)[0])\n",
    "        #rolloff_05.append(librosa.feature.spectral_rolloff(y=signal, sr=300, n_fft=255, hop_length=1, roll_percent=0.05)[0])\n",
    "        #power_spec = librosa.amplitude_to_db(np.abs(librosa.stft(signal, n_fft=255, hop_length=1)), ref=np.max)\n",
    "        #m2.append(scipy.stats.moment(power_spec, moment=2, axis=0))\n",
    "        #m3.append(scipy.stats.moment(power_spec, moment=3, axis=0))\n",
    "        #m4.append(scipy.stats.moment(power_spec, moment=4, axis=0))\n",
    "        \n",
    "    collect = [raw]\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we load the features, we split the data into test and training set at a roughly 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = t.iloc[0:4000, :]\n",
    "t_val = t.iloc[4000:-1, :]\n",
    "y_train = y[0:4000]\n",
    "y_val = y[4000:-1]\n",
    "print(t_train.shape, y_train.shape)\n",
    "print(t_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we normalize the raw signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(t_train)\n",
    "t_train = scaler.transform(t_train)\n",
    "t_val = scaler.transform(t_val)\n",
    "print(t_train.shape, t_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the features for each set and check if the dimensions are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "train_loaded_features = load_features(t_train)\n",
    "val_loaded_features = load_features(t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, stack the features into a 3D array with dimensions [samples, time, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.dstack(train_loaded_features)\n",
    "X_val = np.dstack(val_loaded_features)\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels need to be one-hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val)\n",
    "print(y_train_onehot.shape, y_val_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the 1D-CNN\n",
    "The function below fits and evaluates the model. The function was taken from this great [tutorial](https://machinelearningmastery.com/cnn-models-for-human-activity-recognition-time-series-classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "METRICS = [tf.keras.metrics.TruePositives(name='tp'),\n",
    "           tf.keras.metrics.FalsePositives(name='fp'),\n",
    "           tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "           tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "           tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "           tf.keras.metrics.Precision(name='precision'),\n",
    "           tf.keras.metrics.Recall(name='recall')]\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=4,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(trainX, trainy):\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-3), metrics=METRICS)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train, y_train_onehot)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx7ND3_SqckO"
   },
   "source": [
    "Test run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LopSd-yQqO3a"
   },
   "outputs": [],
   "source": [
    "model.predict(X_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EJj9ixKVBMT"
   },
   "source": [
    "To make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tSUm4yAVIif"
   },
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set weights according to the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/nr_classes helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / counts[0])*(sum(counts))/4.0 \n",
    "weight_for_1 = (1 / counts[1])*(sum(counts))/4.0\n",
    "weight_for_2 = (1 / counts[2])*(sum(counts))/4.0\n",
    "weight_for_3 = (1 / counts[3])*(sum(counts))/4.0\n",
    "\n",
    "CLASS_WEIGHTS = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))\n",
    "print('Weight for class 3: {:.2f}'.format(weight_for_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train, y_train_onehot)\n",
    "model.load_weights(initial_weights)\n",
    "weighted_history = model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_val, y_val_onehot, batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(name, ': ', value)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prediction = model.predict(X_val)\n",
    "int_val_prediction = np.argmax(val_prediction, axis=1)\n",
    "sklearn.metrics.f1_score(y_val, int_val_prediction, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = t.copy()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also load and process the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('./data/X_test.csv')\n",
    "X_test = X_test.drop(['id'], axis=1)\n",
    "X_test = X_test.replace(r'\\\\n', np.nan, regex=True)\n",
    "X_test = X_test.fillna(0)\n",
    "X_test = X_test.iloc[:, 0:9000]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we normalize the raw signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the features and check if the dimensions are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_features(X)\n",
    "X_test = load_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, stack the features into a 3D array with dimensions [samples, time, features]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.dstack(X)\n",
    "X_test = np.dstack(X_test)\n",
    "print(X.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels need to be one-hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = tf.keras.utils.to_categorical(y)\n",
    "print(y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "collect_prediction = list()\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val)\n",
    "    \n",
    "    model = build_model(X_train, y_train)\n",
    "    model.load_weights(initial_weights)\n",
    "    weighted_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=CLASS_WEIGHTS,\n",
    "    verbose=0)\n",
    "    \n",
    "    results = model.evaluate(X_val, y_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "    print('loss : ', results[0])\n",
    "    val_prediction = model.predict(X_val)\n",
    "    int_prediction = np.argmax(val_prediction, axis=1)\n",
    "    int_label = np.argmax(y_val, axis=1)\n",
    "    f1 = sklearn.metrics.f1_score(int_label, int_prediction, average='micro')\n",
    "    print('f1 : ', f1)\n",
    "    print('')\n",
    "    \n",
    "    collect_prediction.append(np.argmax((model.predict(X_test), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = np.argmax(collect_prediction[0], axis=1)\n",
    "final_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.array(range(len(final_prediction)))\n",
    "df = pd.DataFrame({'id': ID,\n",
    "                    'y': final_prediction})\n",
    "name = '1D-CNN_raw.csv'\n",
    "path = os.path.join('.', name)\n",
    "df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentspace",
   "language": "python",
   "name": "latentspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
