{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edited notebook from tensorflow [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) on working with imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2: DISEASE CLASSIFICATION FROM IMAGE FEATURES\n",
    "\n",
    "This task is primarily concerned with multi-class classification where you have 3 classes. However, we have changed the original image features in several ways. You will need to deal with class imbalance; in the training set, there are 600 examples from class 0 and 2 but 3600 examples from class 1. Test set has the same class imbalance as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRHmSyHxEIhN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JM7hDSNClfoK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "c8o1FHzD-_y_"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3iZVjziKHmX"
   },
   "source": [
    "## Data processing and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISEASE CLASSIFICATION FROM IMAGE FEATURES\n",
    "\n",
    "This task is primarily concerned with multi-class classification where you have 3 classes. However, we have changed the original image features in several ways. You will need to deal with class imbalance; in the training set, there are 600 examples from class 0 and 2 but 3600 examples from class 1. Test set has the same class imbalance as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pR_SnbMArXr7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.498973</td>\n",
       "      <td>1.448201</td>\n",
       "      <td>2.784979</td>\n",
       "      <td>1.905992</td>\n",
       "      <td>1.285007</td>\n",
       "      <td>-0.571679</td>\n",
       "      <td>1.253798</td>\n",
       "      <td>-2.590709</td>\n",
       "      <td>1.379211</td>\n",
       "      <td>-1.553323</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.365574</td>\n",
       "      <td>2.856497</td>\n",
       "      <td>-1.916006</td>\n",
       "      <td>1.406900</td>\n",
       "      <td>-2.581604</td>\n",
       "      <td>0.839115</td>\n",
       "      <td>-0.004883</td>\n",
       "      <td>3.173465</td>\n",
       "      <td>2.179183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.521470</td>\n",
       "      <td>-0.493049</td>\n",
       "      <td>0.891382</td>\n",
       "      <td>-0.080855</td>\n",
       "      <td>0.227825</td>\n",
       "      <td>-0.167394</td>\n",
       "      <td>-0.426608</td>\n",
       "      <td>0.371071</td>\n",
       "      <td>-0.065361</td>\n",
       "      <td>-0.271039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443464</td>\n",
       "      <td>-0.540985</td>\n",
       "      <td>-0.164082</td>\n",
       "      <td>0.223598</td>\n",
       "      <td>0.274742</td>\n",
       "      <td>-0.122392</td>\n",
       "      <td>0.971394</td>\n",
       "      <td>0.604963</td>\n",
       "      <td>0.355499</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417724</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>0.938377</td>\n",
       "      <td>-0.670472</td>\n",
       "      <td>0.298922</td>\n",
       "      <td>0.917788</td>\n",
       "      <td>0.189585</td>\n",
       "      <td>-0.259406</td>\n",
       "      <td>0.591056</td>\n",
       "      <td>-1.391407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543036</td>\n",
       "      <td>-0.321695</td>\n",
       "      <td>-1.778676</td>\n",
       "      <td>1.118608</td>\n",
       "      <td>-0.937445</td>\n",
       "      <td>-0.239242</td>\n",
       "      <td>0.842709</td>\n",
       "      <td>2.086818</td>\n",
       "      <td>0.512741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.471972</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.784836</td>\n",
       "      <td>1.088817</td>\n",
       "      <td>-0.436292</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.611958</td>\n",
       "      <td>-0.720903</td>\n",
       "      <td>0.310497</td>\n",
       "      <td>-0.703081</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.942345</td>\n",
       "      <td>0.366181</td>\n",
       "      <td>-1.226904</td>\n",
       "      <td>1.152494</td>\n",
       "      <td>-0.104389</td>\n",
       "      <td>0.702375</td>\n",
       "      <td>0.426496</td>\n",
       "      <td>0.336746</td>\n",
       "      <td>1.304973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.201026</td>\n",
       "      <td>-0.579901</td>\n",
       "      <td>0.638809</td>\n",
       "      <td>-0.614121</td>\n",
       "      <td>0.468388</td>\n",
       "      <td>0.535726</td>\n",
       "      <td>0.271890</td>\n",
       "      <td>0.054270</td>\n",
       "      <td>0.297078</td>\n",
       "      <td>-0.677568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203202</td>\n",
       "      <td>-0.150227</td>\n",
       "      <td>-0.026890</td>\n",
       "      <td>0.106822</td>\n",
       "      <td>-1.202451</td>\n",
       "      <td>0.098924</td>\n",
       "      <td>0.825237</td>\n",
       "      <td>1.044778</td>\n",
       "      <td>0.071464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0 -1.498973  1.448201  2.784979  1.905992  1.285007 -0.571679  1.253798   \n",
       "1 -0.521470 -0.493049  0.891382 -0.080855  0.227825 -0.167394 -0.426608   \n",
       "2 -0.417724 -0.019106  0.938377 -0.670472  0.298922  0.917788  0.189585   \n",
       "3 -0.471972  0.000398  0.784836  1.088817 -0.436292  0.023086  0.611958   \n",
       "4  0.201026 -0.579901  0.638809 -0.614121  0.468388  0.535726  0.271890   \n",
       "\n",
       "         x7        x8        x9  ...      x991      x992      x993      x994  \\\n",
       "0 -2.590709  1.379211 -1.553323  ... -1.365574  2.856497 -1.916006  1.406900   \n",
       "1  0.371071 -0.065361 -0.271039  ... -0.443464 -0.540985 -0.164082  0.223598   \n",
       "2 -0.259406  0.591056 -1.391407  ... -0.543036 -0.321695 -1.778676  1.118608   \n",
       "3 -0.720903  0.310497 -0.703081  ... -1.942345  0.366181 -1.226904  1.152494   \n",
       "4  0.054270  0.297078 -0.677568  ...  0.203202 -0.150227 -0.026890  0.106822   \n",
       "\n",
       "       x995      x996      x997      x998      x999  y  \n",
       "0 -2.581604  0.839115 -0.004883  3.173465  2.179183  1  \n",
       "1  0.274742 -0.122392  0.971394  0.604963  0.355499  0  \n",
       "2 -0.937445 -0.239242  0.842709  2.086818  0.512741  1  \n",
       "3 -0.104389  0.702375  0.426496  0.336746  1.304973  1  \n",
       "4 -1.202451  0.098924  0.825237  1.044778  0.071464  1  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_test = pd.read_csv('./data/X_test.csv')\n",
    "#X_train = X_train.drop(columns='id').to_numpy()\n",
    "#y_train = y_train.drop(columns='id').to_numpy()[:, 0]\n",
    "#X_test = X_test.drop(columns='id').to_numpy()\n",
    "\n",
    "X_train = pd.read_csv('./data/X_train.csv')\n",
    "y_train = pd.read_csv('./data/y_train.csv')\n",
    "X_train['y'] = y_train['y']\n",
    "raw_df = X_train.drop(['id'], axis=1)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-fgdQgmwUFuj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x990</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.403221</td>\n",
       "      <td>0.220492</td>\n",
       "      <td>1.106921</td>\n",
       "      <td>0.068664</td>\n",
       "      <td>0.279619</td>\n",
       "      <td>0.388415</td>\n",
       "      <td>0.236717</td>\n",
       "      <td>-0.152921</td>\n",
       "      <td>0.432778</td>\n",
       "      <td>-0.752647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673128</td>\n",
       "      <td>-0.444427</td>\n",
       "      <td>0.448460</td>\n",
       "      <td>-0.693521</td>\n",
       "      <td>0.540356</td>\n",
       "      <td>-0.729529</td>\n",
       "      <td>0.395605</td>\n",
       "      <td>0.440266</td>\n",
       "      <td>1.240888</td>\n",
       "      <td>0.937046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.663232</td>\n",
       "      <td>0.764817</td>\n",
       "      <td>0.794665</td>\n",
       "      <td>0.888101</td>\n",
       "      <td>0.690293</td>\n",
       "      <td>0.628839</td>\n",
       "      <td>0.625738</td>\n",
       "      <td>0.782704</td>\n",
       "      <td>0.568082</td>\n",
       "      <td>0.573664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866030</td>\n",
       "      <td>0.706408</td>\n",
       "      <td>0.786608</td>\n",
       "      <td>1.033146</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>0.729511</td>\n",
       "      <td>0.584042</td>\n",
       "      <td>0.574004</td>\n",
       "      <td>0.668117</td>\n",
       "      <td>0.884328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.026434</td>\n",
       "      <td>-2.177817</td>\n",
       "      <td>-0.993103</td>\n",
       "      <td>-2.958118</td>\n",
       "      <td>-1.992190</td>\n",
       "      <td>-2.229151</td>\n",
       "      <td>-1.620785</td>\n",
       "      <td>-3.842304</td>\n",
       "      <td>-2.046807</td>\n",
       "      <td>-3.980367</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.351814</td>\n",
       "      <td>-3.587288</td>\n",
       "      <td>-2.027416</td>\n",
       "      <td>-4.932804</td>\n",
       "      <td>-1.548640</td>\n",
       "      <td>-3.720113</td>\n",
       "      <td>-1.686527</td>\n",
       "      <td>-2.076105</td>\n",
       "      <td>-1.547162</td>\n",
       "      <td>-1.861970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.827374</td>\n",
       "      <td>-0.306275</td>\n",
       "      <td>0.518833</td>\n",
       "      <td>-0.604431</td>\n",
       "      <td>-0.180091</td>\n",
       "      <td>-0.002606</td>\n",
       "      <td>-0.188781</td>\n",
       "      <td>-0.681943</td>\n",
       "      <td>0.096057</td>\n",
       "      <td>-1.084700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>-0.854477</td>\n",
       "      <td>-0.152561</td>\n",
       "      <td>-1.446615</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>-1.209385</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.107948</td>\n",
       "      <td>0.773387</td>\n",
       "      <td>0.276949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.312958</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.933785</td>\n",
       "      <td>-0.061674</td>\n",
       "      <td>0.199699</td>\n",
       "      <td>0.357911</td>\n",
       "      <td>0.147484</td>\n",
       "      <td>-0.049152</td>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.696674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473798</td>\n",
       "      <td>-0.344868</td>\n",
       "      <td>0.325952</td>\n",
       "      <td>-0.603889</td>\n",
       "      <td>0.457354</td>\n",
       "      <td>-0.640468</td>\n",
       "      <td>0.335444</td>\n",
       "      <td>0.454828</td>\n",
       "      <td>1.138441</td>\n",
       "      <td>0.752086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.076125</td>\n",
       "      <td>0.624882</td>\n",
       "      <td>1.585165</td>\n",
       "      <td>0.737608</td>\n",
       "      <td>0.644108</td>\n",
       "      <td>0.753130</td>\n",
       "      <td>0.612462</td>\n",
       "      <td>0.409195</td>\n",
       "      <td>0.752769</td>\n",
       "      <td>-0.361554</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246985</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>0.971055</td>\n",
       "      <td>0.111399</td>\n",
       "      <td>0.978256</td>\n",
       "      <td>-0.192801</td>\n",
       "      <td>0.727108</td>\n",
       "      <td>0.792661</td>\n",
       "      <td>1.625235</td>\n",
       "      <td>1.549565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.769423</td>\n",
       "      <td>4.102255</td>\n",
       "      <td>4.946383</td>\n",
       "      <td>3.461174</td>\n",
       "      <td>3.682944</td>\n",
       "      <td>3.209605</td>\n",
       "      <td>3.303836</td>\n",
       "      <td>2.223731</td>\n",
       "      <td>3.235681</td>\n",
       "      <td>0.823849</td>\n",
       "      <td>...</td>\n",
       "      <td>5.333272</td>\n",
       "      <td>1.840349</td>\n",
       "      <td>3.701539</td>\n",
       "      <td>2.378358</td>\n",
       "      <td>3.393483</td>\n",
       "      <td>2.271806</td>\n",
       "      <td>3.589027</td>\n",
       "      <td>2.841595</td>\n",
       "      <td>5.489396</td>\n",
       "      <td>4.947546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x0           x1           x2           x3           x4  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean     -0.403221     0.220492     1.106921     0.068664     0.279619   \n",
       "std       0.663232     0.764817     0.794665     0.888101     0.690293   \n",
       "min      -3.026434    -2.177817    -0.993103    -2.958118    -1.992190   \n",
       "25%      -0.827374    -0.306275     0.518833    -0.604431    -0.180091   \n",
       "50%      -0.312958     0.084477     0.933785    -0.061674     0.199699   \n",
       "75%       0.076125     0.624882     1.585165     0.737608     0.644108   \n",
       "max       1.769423     4.102255     4.946383     3.461174     3.682944   \n",
       "\n",
       "                x5           x6           x7           x8           x9  ...  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000  ...   \n",
       "mean      0.388415     0.236717    -0.152921     0.432778    -0.752647  ...   \n",
       "std       0.628839     0.625738     0.782704     0.568082     0.573664  ...   \n",
       "min      -2.229151    -1.620785    -3.842304    -2.046807    -3.980367  ...   \n",
       "25%      -0.002606    -0.188781    -0.681943     0.096057    -1.084700  ...   \n",
       "50%       0.357911     0.147484    -0.049152     0.409125    -0.696674  ...   \n",
       "75%       0.753130     0.612462     0.409195     0.752769    -0.361554  ...   \n",
       "max       3.209605     3.303836     2.223731     3.235681     0.823849  ...   \n",
       "\n",
       "              x990         x991         x992         x993         x994  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean      0.673128    -0.444427     0.448460    -0.693521     0.540356   \n",
       "std       0.866030     0.706408     0.786608     1.033146     0.707031   \n",
       "min      -1.351814    -3.587288    -2.027416    -4.932804    -1.548640   \n",
       "25%       0.027572    -0.854477    -0.152561    -1.446615     0.039625   \n",
       "50%       0.473798    -0.344868     0.325952    -0.603889     0.457354   \n",
       "75%       1.246985     0.048537     0.971055     0.111399     0.978256   \n",
       "max       5.333272     1.840349     3.701539     2.378358     3.393483   \n",
       "\n",
       "              x995         x996         x997         x998         x999  \n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000  \n",
       "mean     -0.729529     0.395605     0.440266     1.240888     0.937046  \n",
       "std       0.729511     0.584042     0.574004     0.668117     0.884328  \n",
       "min      -3.720113    -1.686527    -2.076105    -1.547162    -1.861970  \n",
       "25%      -1.209385     0.012024     0.107948     0.773387     0.276949  \n",
       "50%      -0.640468     0.335444     0.454828     1.138441     0.752086  \n",
       "75%      -0.192801     0.727108     0.792661     1.625235     1.549565  \n",
       "max       2.271806     3.589027     2.841595     5.489396     4.947546  \n",
       "\n",
       "[8 rows x 1000 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = raw_df[raw_df.drop(['y'], axis=1).columns.tolist()].describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the statistics about all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Stand divs.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "means = desc.loc['mean', :]\n",
    "stds = desc.loc['std', :]\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].hist(means, bins=200)\n",
    "axs[0].set_title('Means')\n",
    "axs[1].hist(stds, bins=200)\n",
    "axs[1].set_title('Stand divs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKB_CVZFLpB"
   },
   "source": [
    "### Examine the class label imbalance\n",
    "\n",
    "Let's look at the dataset imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HCJFrtuY2iLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:\n",
      "    Total: 4800\n",
      "       Class 0: 600 (12.50% of total)\n",
      "       Class 1: 3600 (75.00% of total)\n",
      "       Class 2: 600 (12.50% of total)\n"
     ]
    }
   ],
   "source": [
    "cls0, cls1, cls2 = np.bincount(raw_df['y'])\n",
    "total = cls0 + cls1 + cls2\n",
    "print('Samples:\\n    Total: {}\\n \\\n",
    "      Class 0: {} ({:.2f}% of total)\\n \\\n",
    "      Class 1: {} ({:.2f}% of total)\\n \\\n",
    "      Class 2: {} ({:.2f}% of total)'.format(total, cls0, 100*cls0/total, cls1, 100*cls1/total, cls2, 100*cls2/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x990</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>3600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.526603</td>\n",
       "      <td>0.370680</td>\n",
       "      <td>1.231867</td>\n",
       "      <td>0.304272</td>\n",
       "      <td>0.356849</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.321503</td>\n",
       "      <td>-0.324532</td>\n",
       "      <td>0.406335</td>\n",
       "      <td>-0.781700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868851</td>\n",
       "      <td>-0.578076</td>\n",
       "      <td>0.623741</td>\n",
       "      <td>-0.988952</td>\n",
       "      <td>0.688925</td>\n",
       "      <td>-0.897889</td>\n",
       "      <td>0.464186</td>\n",
       "      <td>0.387649</td>\n",
       "      <td>1.323684</td>\n",
       "      <td>1.127765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.674590</td>\n",
       "      <td>0.782279</td>\n",
       "      <td>0.835823</td>\n",
       "      <td>0.851349</td>\n",
       "      <td>0.723261</td>\n",
       "      <td>0.666483</td>\n",
       "      <td>0.650588</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.594452</td>\n",
       "      <td>0.602392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876593</td>\n",
       "      <td>0.714070</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.964924</td>\n",
       "      <td>0.702516</td>\n",
       "      <td>0.718114</td>\n",
       "      <td>0.609545</td>\n",
       "      <td>0.595501</td>\n",
       "      <td>0.691186</td>\n",
       "      <td>0.898364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.026434</td>\n",
       "      <td>-2.177817</td>\n",
       "      <td>-0.993103</td>\n",
       "      <td>-2.469009</td>\n",
       "      <td>-1.992190</td>\n",
       "      <td>-2.229151</td>\n",
       "      <td>-1.503521</td>\n",
       "      <td>-3.842304</td>\n",
       "      <td>-2.046807</td>\n",
       "      <td>-3.980367</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.351814</td>\n",
       "      <td>-3.587288</td>\n",
       "      <td>-1.488013</td>\n",
       "      <td>-4.932804</td>\n",
       "      <td>-1.456689</td>\n",
       "      <td>-3.720113</td>\n",
       "      <td>-1.571218</td>\n",
       "      <td>-2.076105</td>\n",
       "      <td>-0.491268</td>\n",
       "      <td>-1.861970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.981870</td>\n",
       "      <td>-0.170477</td>\n",
       "      <td>0.590552</td>\n",
       "      <td>-0.378589</td>\n",
       "      <td>-0.143841</td>\n",
       "      <td>-0.012948</td>\n",
       "      <td>-0.127193</td>\n",
       "      <td>-0.865472</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>-1.131507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184201</td>\n",
       "      <td>-1.003178</td>\n",
       "      <td>0.017671</td>\n",
       "      <td>-1.676078</td>\n",
       "      <td>0.188541</td>\n",
       "      <td>-1.370770</td>\n",
       "      <td>0.050548</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.840938</td>\n",
       "      <td>0.429652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.455470</td>\n",
       "      <td>0.239445</td>\n",
       "      <td>1.097984</td>\n",
       "      <td>0.275869</td>\n",
       "      <td>0.283138</td>\n",
       "      <td>0.374925</td>\n",
       "      <td>0.251568</td>\n",
       "      <td>-0.290694</td>\n",
       "      <td>0.395982</td>\n",
       "      <td>-0.729032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750036</td>\n",
       "      <td>-0.484665</td>\n",
       "      <td>0.558826</td>\n",
       "      <td>-0.948979</td>\n",
       "      <td>0.627507</td>\n",
       "      <td>-0.822240</td>\n",
       "      <td>0.418720</td>\n",
       "      <td>0.400556</td>\n",
       "      <td>1.223110</td>\n",
       "      <td>1.020277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.012319</td>\n",
       "      <td>0.805148</td>\n",
       "      <td>1.794967</td>\n",
       "      <td>0.938278</td>\n",
       "      <td>0.758491</td>\n",
       "      <td>0.796119</td>\n",
       "      <td>0.732364</td>\n",
       "      <td>0.249748</td>\n",
       "      <td>0.759247</td>\n",
       "      <td>-0.361592</td>\n",
       "      <td>...</td>\n",
       "      <td>1.465125</td>\n",
       "      <td>-0.084447</td>\n",
       "      <td>1.170087</td>\n",
       "      <td>-0.231851</td>\n",
       "      <td>1.142182</td>\n",
       "      <td>-0.375583</td>\n",
       "      <td>0.809430</td>\n",
       "      <td>0.766213</td>\n",
       "      <td>1.738092</td>\n",
       "      <td>1.774704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.476208</td>\n",
       "      <td>4.102255</td>\n",
       "      <td>4.946383</td>\n",
       "      <td>3.461174</td>\n",
       "      <td>3.682944</td>\n",
       "      <td>3.209605</td>\n",
       "      <td>3.303836</td>\n",
       "      <td>1.996396</td>\n",
       "      <td>3.131946</td>\n",
       "      <td>0.818628</td>\n",
       "      <td>...</td>\n",
       "      <td>5.333272</td>\n",
       "      <td>1.488579</td>\n",
       "      <td>3.701539</td>\n",
       "      <td>1.980672</td>\n",
       "      <td>3.393483</td>\n",
       "      <td>2.271806</td>\n",
       "      <td>3.589027</td>\n",
       "      <td>2.841595</td>\n",
       "      <td>5.489396</td>\n",
       "      <td>4.947546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x0           x1           x2           x3           x4  \\\n",
       "count  3600.000000  3600.000000  3600.000000  3600.000000  3600.000000   \n",
       "mean     -0.526603     0.370680     1.231867     0.304272     0.356849   \n",
       "std       0.674590     0.782279     0.835823     0.851349     0.723261   \n",
       "min      -3.026434    -2.177817    -0.993103    -2.469009    -1.992190   \n",
       "25%      -0.981870    -0.170477     0.590552    -0.378589    -0.143841   \n",
       "50%      -0.455470     0.239445     1.097984     0.275869     0.283138   \n",
       "75%      -0.012319     0.805148     1.794967     0.938278     0.758491   \n",
       "max       1.476208     4.102255     4.946383     3.461174     3.682944   \n",
       "\n",
       "                x5           x6           x7           x8           x9  ...  \\\n",
       "count  3600.000000  3600.000000  3600.000000  3600.000000  3600.000000  ...   \n",
       "mean      0.399517     0.321503    -0.324532     0.406335    -0.781700  ...   \n",
       "std       0.666483     0.650588     0.778846     0.594452     0.602392  ...   \n",
       "min      -2.229151    -1.503521    -3.842304    -2.046807    -3.980367  ...   \n",
       "25%      -0.012948    -0.127193    -0.865472     0.043864    -1.131507  ...   \n",
       "50%       0.374925     0.251568    -0.290694     0.395982    -0.729032  ...   \n",
       "75%       0.796119     0.732364     0.249748     0.759247    -0.361592  ...   \n",
       "max       3.209605     3.303836     1.996396     3.131946     0.818628  ...   \n",
       "\n",
       "              x990         x991         x992         x993         x994  \\\n",
       "count  3600.000000  3600.000000  3600.000000  3600.000000  3600.000000   \n",
       "mean      0.868851    -0.578076     0.623741    -0.988952     0.688925   \n",
       "std       0.876593     0.714070     0.780369     0.964924     0.702516   \n",
       "min      -1.351814    -3.587288    -1.488013    -4.932804    -1.456689   \n",
       "25%       0.184201    -1.003178     0.017671    -1.676078     0.188541   \n",
       "50%       0.750036    -0.484665     0.558826    -0.948979     0.627507   \n",
       "75%       1.465125    -0.084447     1.170087    -0.231851     1.142182   \n",
       "max       5.333272     1.488579     3.701539     1.980672     3.393483   \n",
       "\n",
       "              x995         x996         x997         x998         x999  \n",
       "count  3600.000000  3600.000000  3600.000000  3600.000000  3600.000000  \n",
       "mean     -0.897889     0.464186     0.387649     1.323684     1.127765  \n",
       "std       0.718114     0.609545     0.595501     0.691186     0.898364  \n",
       "min      -3.720113    -1.571218    -2.076105    -0.491268    -1.861970  \n",
       "25%      -1.370770     0.050548     0.016125     0.840938     0.429652  \n",
       "50%      -0.822240     0.418720     0.400556     1.223110     1.020277  \n",
       "75%      -0.375583     0.809430     0.766213     1.738092     1.774704  \n",
       "max       2.271806     3.589027     2.841595     5.489396     4.947546  \n",
       "\n",
       "[8 rows x 1000 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_df = raw_df.copy()\n",
    "\n",
    "# get labels\n",
    "labels = np.array(stat_df['y'])\n",
    "\n",
    "# get boolean arrays to split classes\n",
    "cls0_bool = labels == 0\n",
    "cls1_bool = labels == 1\n",
    "cls2_bool = labels == 2\n",
    "\n",
    "# split classes\n",
    "cls0_df = stat_df[cls0_bool]\n",
    "cls1_df = stat_df[cls1_bool]\n",
    "cls2_df = stat_df[cls2_bool]\n",
    "\n",
    "# get statistics from the different classes\n",
    "desc_cls0 = cls0_df[cls0_df.drop(['y'], axis=1).columns.tolist()].describe()\n",
    "desc_cls1 = cls1_df[cls1_df.drop(['y'], axis=1).columns.tolist()].describe()\n",
    "desc_cls2 = cls2_df[cls2_df.drop(['y'], axis=1).columns.tolist()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the statistics about the features from the different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAALKCAYAAAArsyBpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzde5hsdX3n+/dHLmIiBJCtQWC7jaJBHcXMDjKaY3pQJ2h8BCdqdIyBhGTHic7oiWcicZ4TczETPOdEjBMTsw2GbcZwiZqBMTFKkI5jEsGNIopouHhDEDYCAjGiwPf8UauxaKq6q7suqy7v1/P001Vrrar6rlXVv17f+v1+35WqQpIkSZI0Og9qOwBJkiRJmjcmWpIkSZI0YiZakiRJkjRiJlqSJEmSNGImWpIkSZI0YiZakiRJkjRiJlpqRZLlJL/QdhySFoftjqRJs91ZbCZacyTJl5J8J8khq5ZflqSSbGsnsvYleVaSzyf5VpKLkjyq7ZikeWC701uSfZO8tzk+lWSp7ZikeWG701uSY5NckOSWJHuS/EWSQ9uOa5GZaM2fLwIvW7mT5F8BD2kvnPY1DfH7gf8bOBjYDZzTalDSfLHd6e1jwM8AX287EGkO2e480EHATmAb8CjgDuBP2wxo0ZlozZ8/A3626/5JwLu7N0jy4CT/X5KvJLkxyTuSPKRZd1CSDzTfhNza3D6867HLSX47yd8nuSPJh1d/o7TqtU5ovmG6Pck1SY7vsc1jknwkyTeS3JzkPUkO7Fr/+iRfa17vC0me1Sw/Jsnu5rlvTPKWPmH8e+CKqvqLqvo28BvAU5L88HoHU9JAbHdWqarvVNVbq+pjwD2DHUZJG2C7s0pVfbA517m9qr4F/AHwjIGOpsbCRGv+fBw4IMlRSfYCfhr4H6u2eTPwOOBo4LHAYcCvN+seROfbj0cBW4F/ofOH2u0/AD8HPBzYF/i/egWS5Bg6jd5/AQ4Engl8qdemwO8CjwSOAo6gkwyR5PHAq4Efrar9gZ/oeo7fB36/qg4AHgOc2ysO4InAp1fuVNU/A9c0yyUNz3ZH0qTZ7qzvmcAVA26rMdi77QA0Fivf8vwd8HngaysrkgT4ReDJVXVLs+y/AX8O/FpVfQN4X9f2vwNctOr5/7Sq/qlZfy7wgj5xnAK8q6ouaO5/rddGVXU1cHVzd0/zTc0bm/v3AA8GnpBkT1V9qeuh3wUem+SQqrqZTqPby0OBPauWfRPYv8/2kjbOdkfSpNnu9JHkyXSSyhPW21bjY4/WfPozOt/CnMyqbnRgC/B9wKVJbktyG/A3zXKSfF+SP07y5SS3Ax8FDmy+LVrRPd/gW3QSmV6OoNNztKYkD09ydtNdfjudb6QOgfsapdfS+cbnpma7RzYPPYXON1WfT/KJJM/v8xJ3AgesWnYAnbHLkkbDdkfSpNnu9H6dxwIfBF5TVf97vbg0PiZac6iqvkxnkujz6BSB6HYzne7xJ1bVgc3PD1TVSuPxOuDxwNOaLupnNsuziVC+SqeLez2/CxSdb50OoDN5/L7Xq6o/r6ofo9O9X3SGAlBVV1XVy+h06b8ZeG+S7+/x/FcAT1m502zzGOxOl0bGdkfSpNnuPFA6VZX/FvjtqvqzTeyLRshEa36dAhzXzEe6T1XdC7wTOD3JwwGSHJbkJ5pN9qfTMN2W5GC+16W9GWcAP5dOafUHNa/TqwDF/nR6nW5LchidMc40sT0+yXFJHgx8u4ntnmbdzyTZ0uzTbc1Dek06/0vgSUl+Ksl+dLrSL6+qzw+xb5IeyHanSzoT8fdr7u6bZL9mOJOk0bHd+d5zHAZ8BHh7Vb1jiP3RiJhozamquqaqdvdZ/Xo6Y4Q/3nRd/y2db3UA3kqnPOrKGOC/GSKGS+hMIj2dzpyov6PzLc1qvwn8SLPNX3H/b6UeDJzWxPN1Ot/mvKFZdzxwRZI76UwUfWlTVXB1HHuAnwJ+B7gVeBrw0s3ul6TebHce4At0TpYOAz7U3PYaftII2e7czy8APwS8McmdKz+b3S8NL1XVdgySJEmSNFfs0ZIkSZKkETPRkiRJkqQRM9GSJEmSpBEz0ZIkSZKkETPRkiRJkqQR23uSL3bIIYfUtm3bJvmSklp06aWX3lxVW9qMwXZHWhy2OZImba12Z6KJ1rZt29i9u9+lDiTNmyRfbjsG2x1pcdjmSJq0tdodhw5KkiRJ0oiZaEmSJEnSiJloSZIkSdKImWhJkiRJ0ohNtBiG5leWl++7XUtLrcUhafYsZ/m+20u11FockjSM5eUAsLRULUeiaWGPliRJWmhJ9ktySZJPJ7kiyW82yx+d5OIkVyU5J8m+bccqaXaYaEmSpEV3F3BcVT0FOBo4PsmxwJuB06vqSOBW4JQWY5Q0Y0y0JEnSQquOO5u7+zQ/BRwHvLdZvgs4sYXwJM0oEy1JkrTwkuyV5DLgJuAC4Brgtqq6u9nkOuCwPo/dkWR3kt179uyZTMCSpp6JliRpaixn+X7FMaRJqap7qupo4HDgGOCoXpv1eezOqtpeVdu3bNkyzjAlzRATLUlTJ8kRSS5KcmUzMf01zfLfSPK1JJc1P89rO1ZJ86WqbgOWgWOBA5OsVGg+HLi+rbgkzR4TLUnT6G7gdVV1FJ2TnVcleUKz7vSqOrr5+ev2QpQ0L5JsSXJgc/shwLOBK4GLgBc1m50EnNdOhJJmkdfRkjR1quoG4Ibm9h1JrqTP3AhJGoFDgV1J9qLzJfS5VfWBJJ8Dzk7yJuBTwBltBilptphoSZpqSbYBTwUuBp4BvDrJzwK76fR63dpedJLmQVVdTqedWb38WjrztSRpwxw6KGlqJXko8D7gtVV1O/BHwGPoXOfmBuD3+jzOCmATtFLAYlGKWCzSvkqSNs9ES9JUSrIPnSTrPVX1foCqurGpDHYv8E76fNNsBTBJktQ2Ey1JUydJ6MyFuLKq3tK1/NCuzV4IfHbSsUmSJA3COVqSptEzgFcAn2kuIArwBuBlSY6mcy2bLwG/1E54kqRFtLyc+91fWup5aTUJMNHSBGR5+b7btbTUWhyaHVX1MSA9VlnOXZIkzQSHDkqSJEnSiJloSZIkSdKImWhJkiRp4S0v5wFzsKRhrJtoJdkvySVJPp3kiiS/2Sx/dJKLk1yV5Jwk+44/XEmSJEmafoP0aN0FHFdVT6FzkdDjkxwLvBk4vaqOBG4FThlfmJIkTa9hL2LsRZAlaf6sm2hVx53N3X2anwKOA97bLN8FnDiWCCVJkiRpxgw0RyvJXs21bG4CLgCuAW6rqrubTa4DDhtPiJIkSZI0WwZKtKrqnqo6GjgcOAY4qtdmvR6bZEeS3Ul279mzZ/ORauKyvHy/a2ANuk6SJEladBuqOlhVtwHLwLHAgUlWLnh8OHB9n8fsrKrtVbV9y5Ytw8QqSZIkSTNhkKqDW5Ic2Nx+CPBs4ErgIuBFzWYnAeeNK0hJ0mwaVZEIC0VIkmbN3utvwqHAriR70UnMzq2qDyT5HHB2kjcBnwLOGGOc2oSVoX21tNRqHN2mMSZJkqQVK9fSWlrqOStmap9b02fdRKuqLgee2mP5tXTma0mSJEmSumxojpYkSZIkaX0mWpIkSZI0YoPM0ZIkaWqsFMZYqqWxPec4XkPS7FqZWyVthD1akqZOkiOSXJTkyiRXJHlNs/zgJBckuar5fVDbsUqSJPVioiVpGt0NvK6qjqJz3b5XJXkCcCpwYVUdCVzY3JckSZo6JlpqRZaX7yv1Lq1WVTdU1Seb23fQuXbfYcAJwK5ms13Aie1EKEmStDbnaEmaakm20bnExMXAI6rqBugkY0ke3ucxO4AdAFu3bp1MoNqwcV6EuNccq1mYdzULMUqzote8qpXrV43qelaDzt1yjtdiskdL0tRK8lDgfcBrq+r2QR9XVTurantVbd+yZcv4ApQkSerDREvSVEqyD50k6z1V9f5m8Y1JDm3WHwrc1FZ8kiRJa3HooIYyjnlWK89ZS0sjf27NhiQBzgCurKq3dK06HzgJOK35fV4L4UmSJK3LREvSNHoG8ArgM0kua5a9gU6CdW6SU4CvAC9uKT5J0pQZ1byrUdno/K2NzB/rfu5p2V89kImWpKlTVR8D+v2HetYkY9FodBe+2Gyhh80Uz9joY9bafpzFOyRJ88dESw9g2XVJkiRpOBbDkCRJkqQRM9GSJEmSpBFz6KAkSVpoSY4A3g38IHAvsLOqfj/JwcA5wDbgS8BLqurWtuLUYOb54sDzvG/zyERLkjRR01JUYlri0FS4G3hdVX0yyf7ApUkuAE4GLqyq05KcCpwKvL7FOCXNEIcOSpKkhVZVN1TVJ5vbdwBXAocBJwC7ms12ASe2E6GkWWSiJUmS1EiyDXgqcDHwiKq6ATrJGPDw9iKTNGscOqhWbbSU/Mr2tbQ08lgkSYstyUOB9wGvrarbk8HmwyTZAewA2Lp16/gC1EwY1TwqL0o8++zRkiRJCy/JPnSSrPdU1fubxTcmObRZfyhwU6/HVtXOqtpeVdu3bNkymYAlTT17tCRJU2cjhSpmrajFSrxLtdRqHPqedLquzgCurKq3dK06HzgJOK35fV4L4UmaUSZakiRp0T0DeAXwmSSXNcveQCfBOjfJKcBXgBe3FJ+kGWSiJWDwuVIbnVM1Cmu9Zvc6521Jkjajqj4G9JtY86xJxqLvzU1yXpJmnXO0JEmSJGnE7NGSpDnW1nygWZs3NU7OyZKkxWSPliRJkiSN2Lo9WkmOAN4N/CBwL7Czqn4/ycHAOcA24EvAS6rq1vGFqs1q69pTk57P1e/1nLs1e5K8C3g+cFNVPalZ9hvALwJ7ms3eUFV/3U6EkqRpMOw1q9Z6/Kiuh9XmczvPrV2D9GjdDbyuqo4CjgVeleQJwKnAhVV1JHBhc1+SRuFM4Pgey0+vqqObH5MsSZI0tdZNtKrqhqr6ZHP7DuBK4DDgBGBXs9ku4MRxBSlpsVTVR4Fb2o5DkiRpszZUDCPJNuCpwMXAI6rqBugkY0ke3ucxO4AdAFu3bh0mVs2INkrAr8US8HPl1Ul+FthNp6e953Bl253BWKRhfZst6mExEEnSwMUwkjwUeB/w2qq6fdDHVdXOqtpeVdu3bNmymRglCeCPgMcARwM3AL/Xb0PbHUmaDcvL2dB8pZXtN/o4qQ0DJVpJ9qGTZL2nqt7fLL4xyaHN+kOBm8YToiRBVd1YVfdU1b3AO4Fj2o5JkiSpn3UTrSQBzgCurKq3dK06HzipuX0ScN7ow5OkjpUvdhovBD7bViySJEnrGWSO1jOAVwCfSXJZs+wNwGnAuUlOAb4CvHg8IWpUZm2u0rTN9dLkJDkLWAIOSXId8EZgKcnRQNG5pMQvtRagJEnSOtZNtKrqY0C/QbDPGm04kgRV9bIei8+YeCBzpLs4wziLX1gEor/Vx8ZjJambc87mz8DFMCRJkiRJg9lQeXctJofwSZIkSRtjj5YkSZIkjZiJliRJkjSHvN5Yuxw6KElzYhwFL1aec5wFNCZtVEUoej3POApczON7IEmLwERLM8l5Y5IkSZpmDh2UJEmSpBGzR0uSJElTyzlGmlUmWnPIYXWSRq3X/K+2L7jb9utPI+dzSdL0cOigJEmSJI2YiZYkSZIkjZiJliRJkiSNmHO0JEmSNHIrRSyWlmqox2v4YzHse6HNMdGSpDm0mUIR47jgsTZnnIU+fJ8laTIcOihp6iR5V5Kbkny2a9nBSS5IclXz+6A2Y5QkSVqLPVozpLtsey0ttRbHPOlVCn/l2Hq8W3Um8AfAu7uWnQpcWFWnJTm1uf/6FmKTJElal4mWpKlTVR9Nsm3V4hOApeb2LmAZEy1Jmnrd84tW5gitnnPkfKz2rD72zuMaHYcOSpoVj6iqGwCa3w9vOR5JkqS+7NGSNHeS7AB2AGzdurXlaGbTOIsxaHNW3hMLWEjSbDDRmnEr84g2Oodos4+bF73mZmnq3Zjk0Kq6IcmhwE39NqyqncBOgO3btzsGQpIkTZxDByXNivOBk5rbJwHntRiLpDljtdPFsLwc54NpYky0JE2dJGcB/wg8Psl1SU4BTgOek+Qq4DnNfUkalTOB41ctW6l2eiRwYXNfkgbi0EFJU6eqXtZn1bMmGoikhWG1U0mjZqI1JXrNmVr0eVTjsNm5Wb4XmhbdRSo2WxTBQhezzaIYE3W/aqdJrHYqaWAmWpIkSUOw0qlmjfPUJsM5WpIkSb3d2FQ5Za1qp1W1s6q2V9X2LVu2TDRASdNr3R6tJO8Cng/cVFVPapYdDJwDbAO+BLykqm4dX5iLrddwt9XLhh0SJ0mSHmCl2ulpWO1U0gYN0qN1JlbhkSRJc8xqp5JGbd0eLavwSJKkeWe108lwbtDorRzTpaWa6deYR5udo3W/KjyAVXgkSZIkqTH2qoNW4vme7vlQlgmfbb3mtvmeSpIkacVme7QGqsIDVuKRJEmStHg226NlFR5JapkXrpXUlrXmWjmPpx3Of5s+g5R3P4tO4YtDklwHvJFOgnVuU5HnK8CLxxnkPLO8+mzx/ZIkSdIgBqk6aBUeSZIkSdqAzc7RkiRJkiT1Mfaqg5IkSZpdXkNpfgwyj6t7m9Xvea91fj76M9GSpAW3UlRDs2ut93Aj768FViRpdEy0JM2UJF8C7gDuAe6uqu3tRiRJkvRAJlqSZtG/raqb2w5CkiSpHxOtllgmXJIkSZpfJlqSZk0BH05SwB9X1c62A5IkaR55EeThmGhJmjXPqKrrkzwcuCDJ56vqo90bJNkB7ADYunVrGzFOlMUsNCw/Q5I0el5HS9JMqarrm983AX8JHNNjm51Vtb2qtm/ZsmXSIUqSJNmjNU4r87BqaWnDj5H0QEm+H3hQVd3R3P53wG+1HJYkSdIDmGhJmiWPAP4yCXTarz+vqr9pNyRJmk4buZDsWheplbQ5JlqSZkZVXQs8pe041tM932UeL/zqfJ7p53skSe1zjpYkSZIkjZg9WtKIrJ5f1z03bzPz9SRJkjS7TLQkSZL0AJu9hpLXXloMvs/rc+igJEmSJI2YPVrSKuMsse8QwsW1UpygV3GMtdat3kZaMc7PxLwXdJGkSbBHS5IkSZJGzB4tSZKkObbWXJpe18xy7o02Y6Ofs0Vgj5YkSZIkjZg9WhMwzjk/ml7Dvu/dj3dOlyRJ0mwx0ZKkxiBFKUb1Gr1ex4IXmkar/y56fU57rbOIhqRFZ6IlSZI0hVbmvKzMb+meA9Nvzss451c5d0vjsPpzPk8WPtFaPbxro0O0hn28FtNmhxVaHl6SJGk2WAxDkiRJkkbMREuSJEmSRmzhhw5KkrSoNlKAZaPFWlZvv1bBjF6FaCZRnGYarDUPa/U2az1u2NeXxmmjn7N+fxfdn/fVz7nW38JG5zeOar7YUIlWkuOB3wf2Av6kqk4bSVT0nsOyMi9ls2WvB5kXYyl2zYJefwNrfXaHndM1TXPDxtnuSFIvtjuSNmPTQweT7AW8HXgu8ATgZUmeMKrAJGk12x1Jk2a7I2mzhpmjdQxwdVVdW1XfAc4GThhNWJLUk+2OpEmz3ZG0KcMMHTwM+GrX/euAp63eKMkOYEdz984kX9jsC/Ya3bnBkcWHADdv9vUHMcaRzmOPfYyMfUhrfa76rLtf3KP6XG7ieR41opdeMWy7M9j7Oco/5E28eWMwFZ/jFrjfkzLIZ3mQf+LD/aM/hNG3OTBAuzOac53Apt67qZ5jNW9/g/O2PzBV+7TZRuJ+6wbYn802WH31bXeGSbR6RfCAmWNVtRPYOcTrjEyS3VW1ve04NsPY2zGrsc9q3AMYqt2Z4+OyJvd7sSz4fm8bx1P3WHa/dmdU5zrz9t65P9Nv3vZp2vZnmKGD1wFHdN0/HLh+uHAkaU22O5ImzXZH0qYMk2h9AjgyyaOT7Au8FDh/NGFJUk+2O5ImzXZH0qZseuhgVd2d5NXAh+iUO31XVV0xssjGYyqGMG6SsbdjVmOf1bjXNIJ2Zy6PywDc78Xifo/QhM935u29c3+m37zt01TtT6pGc0EuSZIkSVLHMEMHJUmSJEk9mGhJkiRJ0ojNdaKV5MVJrkhyb5K+pR6THJ/kC0muTnLqJGPsJ8nBSS5IclXz+6A+292T5LLmp9XJuesdxyQPTnJOs/7iJNsmH+UDDRD3yUn2dB3nX2gjzl6SvCvJTUk+22d9kryt2bfLk/zIpGOcJkn+3ySfb47FXyY5sO2YJmXQ9nBeTGO7Pm7rtQfzKskRSS5KcmXzGX9N2zENapbPU3qZxXOXXmb1fKafWT7P6WWWzn3mOtECPgv8e+Cj/TZIshfwduC5wBOAlyV5wmTCW9OpwIVVdSRwYXO/l3+pqqObnxdMLrz7G/A4ngLcWlWPBU4H3jzZKB9oA+//OV3H+U8mGuTazgSOX2P9c4Ejm58dwB9NIKZpdgHwpKp6MvBPwK+1HM8krdsezospbtfH7UzWbg/m1d3A66rqKOBY4FUz9H7P8nlKLzN17tLLrJ7P9DMH5zm9nMmMnPvMdaJVVVdW1XpXZz8GuLqqrq2q7wBnAyeMP7p1nQDsam7vAk5sMZZBDHIcu/fpvcCzkrR9Sftpff8HUlUfBW5ZY5MTgHdXx8eBA5McOpnopk9Vfbiq7m7ufpzO9XAWwoDt4byY6b/rzRqgPZhLVXVDVX2yuX0HcCVwWLtRDWbGz1N6mbVzl15m9Xymn1n6/Axkls595jrRGtBhwFe77l/HdDTQj6iqG6DzTwR4eJ/t9kuyO8nHk7TZoA1yHO/bpjnZ/SbwsIlE19+g7/9PNd3P701yRI/102paP9/T4OeBD7YdhMbCz/2CaoZwPRW4uN1IRmqWPs+zdu7Sy6yez/Qz7+c5vUzN38ymr6M1LZL8LfCDPVb916o6b5Cn6LFsIjXv14p9A0+ztaquT/JDwEeSfKaqrhlNhBsyyHFs7VivYZCY/hdwVlXdleSVdL7FOm7skY3GNB7zsRqkTUjyX+kMN3rPJGMbtxG0h/Ni4T73giQPBd4HvLaqbm87nhWzfJ7Sy5ydu/Qyq+cz/cz7eU4vU/P+zHyiVVXPHvIprgO6M/fDgeuHfM6BrBV7khuTHFpVNzTdnTf1eY7rm9/XJlmm801eG43VIMdxZZvrkuwN/ADtD3NZN+6q+kbX3XcyxWOxe2jt892W9dqEJCcBzweeVXN2IcERtIfzYuE+94suyT50kqz3VNX7246n2yyfp/QyZ+cuvczq+Uw/836e08vU/M04dBA+ARyZ5NFJ9gVeCkxDBZzzgZOa2ycBD/jWK8lBSR7c3D4EeAbwuYlFeH+DHMfufXoR8JEpONFdN+5V43pfQGf8/6w4H/jZpgLPscA3V4Z1LKIkxwOvB15QVd9qOx6NzbS26xqDZm7MGcCVVfWWtuMZg1n6PM/auUsvs3o+08+8n+f0Mj3nPlU1tz/AC+lktXcBNwIfapY/Evjrru2eR6cC2TV0uvKnIfaH0anYc1Xz++Bm+XbgT5rbTwc+A3y6+X1KyzE/4DgCv0XnpBZgP+AvgKuBS4Afavs4Dxj37wJXNMf5IuCH2465K/azgBuA7zaf9VOAVwKvbNaHTrWha5rPyPa2Y275eF1NZ9z2Zc3PO9qOaYL73rM9nNefaWzXJ7DPD2gP2o5pQvv9Y3SGBV3e9bf9vLbjGjD2mT1P6bM/M3fu0mc/ZvJ8Zoj9mdrznD77MzPnPmkCkiRJkiSNiEMHJUmSJGnETLQkSZIkacRMtCRJkiRpxEy0JEmSJGnETLQkSZIkacRMtCRJkiRpxEy0JEmSJGnETLQkSZIkacRMtCRJkiRpxEy0JEmSJGnETLQkSZIkacRMtDSQJCcn+dgmHndmkjeNIyZJ88s2R9Kk2e5o1Ey0FkSSH0vyD0m+meSWJH+f5EebdZtqWKZRkqOTXJrkW83vo9uOSVpEC9Tm7EzyhST3Jjm57XikRbYI7U6SxyU5L8meZh8/lOTxbcel3ky0FkCSA4APAP8dOBg4DPhN4K424xq1JPsC5wH/AzgI2AWc1yyXNCGL0uY0Pg38MvDJtgORFtkCtTsHAucDjwceAVxC59xHU8hEazE8DqCqzqqqe6rqX6rqw1V1eZKjgHcA/ybJnUluA0jysCTnJ7k9ySXAY9Z6ga5vkW5L8tVe3+wmOSjJB5pvYW5tbh/etf7kJNcmuSPJF5O8vFn+2CR/13xDdXOSc/qEsQTsDby1qu6qqrcBAY7b6AGTNJRFaXOoqrdX1YXAtzdxnCSNzkK0O1V1SVWdUVW3VNV3gdOBxyd52OYOm8bJRGsx/BNwT5JdSZ6b5KCVFVV1JfBK4B+r6qFVdWCz6u10ThwOBX6++ekpyVbgg3S+RdoCHA1c1mPTBwF/CjwK2Ar8C/AHzXN8P/A24LlVtT/w9K7n+G3gw3R6qQ5vXqeXJwKXV1V1Lbu8WS5pchalzZE0PRa13Xkm8PWq+saA22uCTLQWQFXdDvwYUMA7gT3NNziP6LV9kr2AnwJ+var+uao+S2cYXj8vB/62+Rbpu1X1jap6QOPTLH9fVX2rqu4Afgf48a5N7gWelOQhVXVDVV3RLP8unQbrkVX17arqN8b6ocA3Vy37JrD/GrFLGrEFanMkTYlFbHeanrK3A7+y3rZqh4nWgqiqK6vq5Ko6HHgS8EjgrX0230JnCN5Xu5Z9eY2nPwK4Zr0Yknxfkj9O8uUktwMfBQ5MsldV/TPw03S+cbohyV8l+eHmob9KZwjgJUmuSNLvG6c7gQNWLTsAuGO92CSN1oK0OZKmyCK1O0m20OkB+8OqOmu9uNQOE60FVFWfB86k0whB59ufbnuAu+k0Kiu2rvGUX2Wdcc2N19GZvPm0qjqATnc3dBoWqupDVfUcOl34n6fzjRRV9fWq+sWqeiTwS8AfJnlsj+e/AnhyknQte3KzXFJL5rjNkTSl5rndaYZFfhg4v6p+Z4CY1BITrQWQ5IeTvG5lMmaSI4CXAR9vNrkRODxNdb6qugd4P/AbzTczTwBOWuMl3gM8O8lLkuzdTC7tVVZ9fzpjlW9LcjDwxq4YH5HkBc345bvo9E7d06x7cddE0lvpNJb39Hj+5Wb5f07y4CSvbpZ/ZI3YJY3YArU5JNk3yX50TqL2SV/wT+cAACAASURBVLJfEv+3ShO2KO1OOtUVPwT8fVWdOsChUYv8Z7AY7gCeBlyc5J/pNDqfpfOtC3QSkSuArye5uVn2ajpznr5O5xuhP+335FX1FeB5zfPdQmdi51N6bPpW4CHAzU0Mf9O17kHN469vnuPH6ZRMBvjRJvY76ZQ0fU1VfbFHHN8BTgR+FriNzqTWE5vlkiZnIdqcxofpnFQ9HdjZ3H5mn20ljc+itDsvbLb9uXQqKK78rNUbp5bk/gXaJEmSJEnDskdLkiRJkkbMREuSJEmSRsxES5IkSZJGzERLkiRJkkZs70m+2CGHHFLbtm2b5EtKatGll156c1Vt2cxjk+wF7Aa+VlXPT/Jo4GzgYOCTwCsGqShpuyMtjmHanFGxzZEWy1rtzkQTrW3btrF79+5JvqSkFiX58hAPfw1wJXBAc//NwOlVdXaSdwCnAH+03pPY7kiLY8g2ZyRsc6TFsla749BBSVOnuWjjTwJ/0twPcBzw3maTXXSumSZJkjSVTLQkTaO3Ar8K3NvcfxhwW1Xd3dy/Djis34OT7EiyO8nuPXv2jDdSSZKkHky0JE2VJM8HbqqqS7sX99i079XWq2pnVW2vqu1btrQ6XUOSJC2oic7RkqQBPAN4QZLnAfvRmaP1VuDAJHs3vVqHA9e3GKMkSdKa7NGSNFWq6teq6vCq2ga8FPhIVb0cuAh4UbPZScB5LYUoSZK0Lnu0hpTlZQBqaanVOKQF8Hrg7CRvAj4FnNFyPJuynOX7bi/VUmtxSNJGLS9/bxT30lLf0duSGiZakqZWVS0Dy83ta4Fj2oxHkiRpUA4dlCRJkqQRM9GSpCEtZ/l+QwIlSZJMtCRJkiRpxEy0JEmSJGnETLQkaUQcQihJklaYaEmSJEnSiJlojVGWl++7zpYkSZpuSfZK8qkkH2juPzrJxUmuSnJOkn3bjlHS7DDRkiRJ6ngNcGXX/TcDp1fVkcCtwCmtRCVpJploSdIEOH9Lmm5JDgd+EviT5n6A44D3NpvsAk5sJzpJs8hES5IkCd4K/Cpwb3P/YcBtVXV3c/864LBeD0yyI8nuJLv37Nkz/kglzQQTLUmStNCSPB+4qaou7V7cY9Pq9fiq2llV26tq+5YtW8YSo6TZs3fbAUiSJLXsGcALkjwP2A84gE4P14FJ9m56tQ4Hrm8xRkkzxh4tSZK00Krq16rq8KraBrwU+EhVvRy4CHhRs9lJwHkthShpBg2caFnyVJIkLZjXA7+S5Go6c7bOaDkeSTNkIz1aljyVJElzraqWq+r5ze1rq+qYqnpsVb24qu5qOz5Js2OgRMuSp5IkSZI0uEF7tDZd8lSSJEmSFs26VQe7S54mWVpZ3GPTniVPk+wAdgBs3bp1k2FK0vzrvqDxUi21FockSRreIOXdhyp5WlU7gZ0A27dv75mMSZIkaXYsL3e+c19aWv/UbmXbQbeX5sW6QwcteSpJkiRJGzPMdbQseSpJkiRJPQwydPA+VbUMLDe3rwWOGX1IkjQ/uuddSZKkxTFMj5YkSZIkqQcTLUmSJEkaMRMtSVMnyX5JLkny6SRXJPnNZvmjk1yc5Kok5yTZt+1YJUmSejHRkjSN7gKOq6qnAEcDxyc5FngzcHpVHQncCpzSYoySJEl9mWhJmjrVcWdzd5/mp4DjgPc2y3cBJ7YQniRJ0ro2VHVQkiYlyV7ApcBjgbcD1wC3NRdJB7gOOKzPY3cAOwC2bt06/mBXGUWlwZXnWKqlvs+9ep0kSZoe9mhJmkpVdU9VHQ0cTudSEkf12qzPY3dW1faq2r5ly5ZxhilJktSTiZakqVZVt9G5ft+xwIFJVnriDweubysuSZKktZhoSZo6SbYkObC5/RDg2cCVwEXAi5rNTgLOaydCSdJ6lpfD8nLaDkNqjXO0JE2jQ4FdzTytBwHnVtUHknwOODvJm4BPAWe0GeSw+s3DkiRJs89ES9LUqarLgaf2WH4tnflakiRJU82hgxOQ5WWyvNx2GJIkSZImxERLkiRJkkbMREuSJC20JPsluSTJp5NckeQ3m+WPTnJxkquSnJNk37ZjlTQ7nKPVspUhhbW01GockiQtsLuA46rqziT7AB9L8kHgV4DTq+rsJO8ATgH+qM1A2zBI5cDubZaWel7iUFo49mhJkqSFVh13Nnf3aX4KOA54b7N8F3BiC+FJmlH2aEmSpIXXXE7iUuCxwNuBa4DbquruZpPrgMP6PHYHsANg69at4w92QjbakyXp/uzRkiRJC6+q7qmqo4HD6VxG4qhem/V57M6q2l5V27ds2TLOMCXNkHV7tJLsB3wUeHCz/Xur6o1JHg2cDRwMfBJ4RVV9Z5zBStI8Wrlw8bDbSBpeVd2WZBk4Fjgwyd5Nr9bhwPWtBidppgzSo7UyQfQpwNHA8UmOBd5MZ4LokcCtdCaIag0r19PymlqSJE2PJFuSHNjcfgjwbOBK4CLgRc1mJwHntROhpFm0bqLlBFFJkjTnDgUuSnI58Anggqr6APB64FeSXA08DDijxRglzZiBimEMM0FUkiRpmlXV5cBTeyy/ls58LUnasIESraq6Bzi66Vb/SzYwQXReK/FIkvOmJElSPxuqOlhVtwHLdE0QbVb1nSBqJR5JkiRJi2bdRMsJopIkSZK0MYMMHTwU2NXM03oQcG5VfSDJ54Czk7wJ+BQLNEG0V9VAKwlKmkYrwxuXaqnVOCQtjrUuYryybmmpet6X5sm6iZYTRCVJkiRpYzY0R2vRTPKaV15fS5IkSZofA1UdlCS1w8qGksbJoXvS+NijJUmSJEkjZo/WBji0T5IkSdIg7NGSJEnSTFhezppVDaVpYo+WJM2ojZRu757rZal3SZLGz0RrFYcHSpIkSRqWiZYkSdKCczieNHrO0ZI0dZIckeSiJFcmuSLJa5rlBye5IMlVze+D2o5VkiSpFxOtKbb6IsYr9x3eqAVwN/C6qjoKOBZ4VZInAKcCF1bVkcCFzX1JkqSp49BBSVOnqm4Abmhu35HkSuAw4ARgqdlsF7AMvL6FECVppjlUUBo/E60BjKMHaRK9UiuvUUtLY38taVySbAOeClwMPKJJwqiqG5I8vM9jdgA7ALZu3TrSeLqr902zWYlTkqR55dBBSVMryUOB9wGvrarbB31cVe2squ1VtX3Lli3jC1CSJKkPe7QkTaUk+9BJst5TVe9vFt+Y5NCmN+tQ4Kb2IpQkbdbqoYvd95eWaqSvMarnkzbKREvS1EkS4Azgyqp6S9eq84GTgNOa3+dNKiaH4kmSpI1w6KCkafQM4BXAcUkua36eRyfBek6Sq4DnNPclaSheUkLSONijJWnqVNXHgH4lsZ41yVgkLYSVS0p8Msn+wKVJLgBOpnNJidOSnErnkhJWOp0x4xiWKA3CRGsGeN0sSZLGx0tKSBoHEy1JmhMbnUe2evulWhpZLNKs2swlJSSpl3XnaDluWZIkLYLNXlIiyY4ku5Ps3rNnz/gC1JqWlzOSCzGP6nmkQYphrIxbPgo4FnhVkifQGad8YVUdCVzY3NeQsrx8348kSZqMtS4p0azve0kJr90nqZd1hw46blmSptuwQwalRTeNl5RYZF7/SvNiQ3O0NjNuOckOYAfA1q1bh4lVY9Tdg1ZLS63FIUlSC1YuKfGZJJc1y95AJ8E6N8kpwFeAF7cUn6QZNHCitXrccufLn/VV1U5gJ8D27dv9akKSJE0VLykhaRwGSrTWGrfc9Gb1HbcsSZpPK0MQ16pWOMg2ksZj1ofgbbYgxUYe5zW2NE7rJlqOW549FtKQJEmS2jVIj5bjliVJkiRpAwapOui4ZUmSpBnlNaGkdmyo6qCmzyDDBFe2sZqgpM3qLgnvfCtJktY3yAWLJUmSJEkbsPCJVpaXLR4hSZIkaaQcOihJAqZ/eGC/UvHTHrckaTEtfI+WJEmSJI3aQvVozVNRiGGHO87TsZAkSZKmzUIlWpKk4XUP1dvMYxzeJ0laBA4dlCRJkqQRs0dLD+CwQkmSNM2GvQizF3HWJJhoSZL62swwwWFew2GFkqR54dBBSZIkSRoxe7Q0Et1VEB1yKEmSpEVnj5akqZPkXUluSvLZrmUHJ7kgyVXN74PajFGSJGktC5loZXn5vh/1N6pj5PHWJpwJHL9q2anAhVV1JHBhc1+SJGkqLWSiJWm6VdVHgVtWLT4B2NXc3gWcONGgJElaw/JyrGao+zHRkjQrHlFVNwA0vx/ecjySJEl9LUQxDIesjc6w19jyGl2ahCQ7gB0AW7du3fDjLTc+Gb1Kx68s87hr0pK8C3g+cFNVPalZdjBwDrAN+BLwkqq6ta0YJc0We7QkzYobkxwK0Py+qd+GVbWzqrZX1fYtW7ZMLEBJM+1M5mBu6MrwNYewbdxax81jqs1YN9Gy+pekKXE+cFJz+yTgvBZjkTRnnBsqadQG6dE6kzn4hkej1V1J0KGZGrUkZwH/CDw+yXVJTgFOA56T5CrgOc19qaflLPccmihtkHNDJW3aunO0quqjSbatWnwCsNTc3gUsA68fYVySFlhVvazPqmdNNBBJGsCw80I1OQ7/0yRtdo6W3/BIkqR5N9DcUOeFSupl7FUHZ+VbnkUY/japfbSyoCRpTqzMDT0N54ZK2qDNJlo3Jjm0qm4YpPoXsBNg+/bttcnXkyRN0GbnN416XtQ45ln1ek7LyauZG7oEHJLkOuCNdBKsc5t5ol8BXtxehGtzSNz0WHkvlpY87V10m020/IZngYyzJ8zeL0nSNHBuqKRRG6S8u9W/JEmSJGkDBqk66Dc8kqS+Njq8b5Dt19pmI6/Xva3DAyVtRK/hmBsZFtj9+NXbr7VO82PsxTDasgjFLeZJ9/vlMEJJkiTNurlNtCRJkuadRTDaMY6CF6vfS3u6Zp+JliRpIYyjgqEkSf3MXaLlkMHFYLVCSZIkTbO5S7QkSZLmkQUUpNlioqWpZy+lpElbGWbYXamw1zJJkvpZ9zpakiRJkqSNsUdLkiRpxlhtcLaM+v3q9Xy9hpOOozqiBmeiteDGOSxvreceZN04Cl1YREOSJEmTYKIlSZobw5Zw38jje23ba/7WIHO7nP8lSfPHREuSJEmaMr2qTG52COLqIYRWsJwMEy3NtO4hiBsZDugQQkmSJI2TiZYkNXoNBRt2KJpmQ7/3ebPv/yCfJYcJStJ8M9GSJElqiVXhZtu8V3/08zmcmU+0vJitVls9LHDQz8haj3OIoSRJkjZi5hMtSZImaa3hhKMaatr9PJMcYmj1Q0kanZlItCxcsJgm0VvZ6zUGed1e22ymGMdGHydJmk8O0VLbNvsZ9LPb34PaDkCSJEmS5s1QiVaS45N8IcnVSU4dVVCS1I/tjqRJs92RtBmbHjqYZC/g7cBzgOuATyQ5v6o+N4rANjukS1oxys/LRoavrrXtrA5LnJbhu+Nud6RJGnQ+10bmfa3MrdroXLG15mQNO29rs4+flvlik253RnmRWs2GQd7fjWwzziF8g1zoeK1teu3HIBdRnpZ926hherSOAa6uqmur6jvA2cAJI4lKknqz3ZE0abY7kjZlmETrMOCrXfeva5ZJ0rjY7kiaNNsdSZsyTNXBXn2YD+hnS7ID2NHcvTPJF9Z53kOAm4eIa1a530OaxMCKtV5j9bp14rlvvweJe7P7Nupjsonne9QEQhhFuzMJ8/g37j6N2rB/tL0fv/4+DdKAjSe2zT6u3z6Nus3pF8H92p3xtTkbPmjT9Dc5LbEsYBzrnq2simWzDcDQDUcTx0YeN7azvXWOyZr6tjvDJFrXAUd03T8cuH71RlW1E9g56JMm2V1V24eIaya534tlUfd7BMbS7kzCPL7n7tNscJ+Gtm67My1tzjS919MSi3E80LTEMi1xwPhiGWbo4CeAI5M8Osm+wEuB80cTliT1ZLsjadJsdyRtyqZ7tKrq7iSvBj4E7AW8q6quGFlkkrSK7Y6kSbPdkbRZwwwdpKr+GvjrEcWyovWu95a434tlUfd7aGNqdyZhHt9z92k2uE9DmqF2Z5re62mJxTgeaFpimZY4YEyxpGp89eglSZIkaRENM0dLkiRJktRDa4lWkuOTfCHJ1UlOXWO7FyWpJFNRlWRY6+13kpOT7ElyWfPzC23EOUqDvNdJXpLkc0muSPLnk45xHAZ4r0/vep//KcltbcSp0ZrHz/sAn+WtSS5K8qkklyd5XhtxbkSSdyW5Kcln+6xPkrc1+3x5kh+ZdIwbMcD+vLzZj8uT/EOSp0w6xs1Yb7+6tvvRJPckedGkYpu0Yc6bkvxa87gvJPmJNuJIsi3Jv3T933vHMHEMEsta51RJTkpyVfNzUotx3NO1fOgiK8P8D5rkMVknjpEdk2HOxUZyPKpq4j90JpNeA/wQsC/waeAJPbbbH/go8HFgexuxTnq/gZOBP2g71gnv85HAp4CDmvsPbzvuSez3qu3/E50J1q3H7s943/dZ+7wPuE87gf/Y3H4C8KW24x5gv54J/Ajw2T7rnwd8kM7FVI4FLm475iH35+ldn7nnTvv+DLpfzTZ7AR+hM4fqRW3HPKbjsOnzpuZv8tPAg4FHN8+zVwtxbFvrfRxHLP3OqYCDgWub3wc1tw+adBzNujsnfEx6/g9q4Zj0/V84qmMy6Oe1a/v7zsVGdTza6tE6Bri6qq6tqu8AZwMn9Njut4H/B/j2JIMbo0H3e54Mss+/CLy9qm4FqKqbJhzjOGz0vX4ZcNZEItM4zePnfZB9KuCA5vYP0OPaZtOmqj4K3LLGJicA766OjwMHJjl0MtFt3Hr7U1X/sPKZo3Pye/hEAhvSAO8TdE6O3gdM+9/SMIY5bzoBOLuq7qqqLwJXN8836ThGbZhzqp8ALqiqW5q/iwuA41uIY9SG+R806WMyif+Fw5yLjeR4tJVoHQZ8tev+dc2y+yR5KnBEVX1gkoGN2br73fipZnjHe5Mc0WP9LBlknx8HPC7J3yf5eJLN/mFPk0Hfa5I8is63jB+ZQFwar3n8vA+yT78B/EyS6+j0KvynyYQ2VgP/Dc+gU+j01s28JIcBLwSGHoY25YY5bxrlZ3nY87dHpzPE+O+S/B+bjGHgWBq9zqkmekzWiANgvyS7m/8HJ24yho3E0u9/0KSPyVr/C0d1TIY5FxvJ8RiqvPsQ0mPZfeUPkzwIOJ1OV+s8WXO/G/8LOKuq7krySmAXcNzYIxufQfZ5bzpdyEt0vmX930meVFWzPGdpkP1e8VLgvVV1zxjj0WTM4+d9kH16GXBmVf1ekn8D/FmzT/eOP7yx2cjf8MxI8m/pJFo/1nYsI/JW4PVVdU/S6y2bG8OcN43yszxMHDcAW6vqG0n+NfA/kzyxqm4fRyyNfudUEzsm68QBnWNyfZIfAj6S5DNVdc0YY+n5P2jAx449juZ/4aiOyTDnYiM5Hm31aF0HdGfzh3P/oSb7A08ClpN8ic74+PMz+wUx1ttvquobVXVXc/edwL+eUGzjsu4+N9ucV1XfbYY1fIHOH98sG2S/V7wUhw3Oi3n8vA+yT6cA5wJU1T8C+wGHTCS68dnI3/BMSPJk4E+AE6rqG23HMyLbgbObc4UXAX84gl6BaTTMedMoP8ubjqMZuvgNgKq6lM7cmcdtMo5BYlnrnGqSx2TNc7uqur75fS2wDDx1k3EMFAv9/wdN9JisEccoj8kw52KjOR6bnWA2zA+dLPZaOl10K5PTnrjG9svMRzGMdfcbOLTr9guBj7cd9wT2+XhgV3P7EDpdtQ9rO/Zx73ez3eOBL9Fc086f2f6Zx8/7gPv0QeDk5vZRzT+jqf9Ms8bkfOAnuX8xjEvajnfI/dlKZ27O09uOc5T7tWq7M5nfYhibPm8Cnsj9i2Fcy+aLYQwTx5aV16VTnOBrwMHjPCb9zqnoFDj4Ip0iBwc1tzcVy5BxHAQ8uLl9CHAVaxRrGFEsPf8HtXBM+sUxsmMy6OeVHudiozoerQwdrKq7k7wa+BCdiiDvqqorkvwWsLuqhi5vOY0G3O//nOQFwN10JgCf3FrAIzDgPn8I+HdJPgfcA/yXmvFvWzfwGX8ZnUnKMz8kSfP5eR9wn14HvDPJ/0lnaMXJ0/6ZTnIWnSErhzRzy94I7ANQVe+gM9fseXSSk28BP9dOpIMZYH9+nc5JzB82Q+zurqqpHyUywH4thGHOm5rtzgU+R+fc4lW1yaHqQ56/PRP4rSR302n7XllV6xU6GTaWnudUVXVLkt8GPtE83W9tNpYhz+2OAv44yb10RpmdVlWf20wcG4il7/+gCR+TnnEkeTojOibDnIuN6jOSKf9fKEmSJEkzp7ULFkuSJEnSvDLRkiRJkqQRM9GSJEmSpBEz0ZIkSZKkETPRkiRJkqQRM9GSJEmSpBEz0ZIkSZKkETPRkiRJkqQRM9GSJEmSpBEz0ZIkSZKkETPRkiRJkqQRM9HSRCQ5M8mb2o5D0uKw3ZE0abY76maiNceSfCnJd5Icsmr5ZUkqybZ2Ihu/JEcnuTTJt5rfR7cdk7QIFrzd2ZnkC0nuTXJy2/FIi2JR250kj0tyXpI9SW5J8qEkj287Ln2Pidb8+yLwspU7Sf4V8JD2whm/JPsC5wH/AzgI2AWc1yyXNH4L1+40Pg38MvDJtgORFtAitjsHAucDjwceAVxC5/xHU8JEa/79GfCzXfdPAt7dvUGSn0zyqSS3J/lqkt/oWvfTSa5NckBz/7lJvp5kS68XS/JjSf4hyW3Nc53cY5uDknyg+Qbm1ub24V3rT25e844kX0zy8mb5Y5P8XZJvJrk5yTl99nkJ2Bt4a1XdVVVvAwIct+7RkjQKi9juUFVvr6oLgW8PcpAkjdTCtTtVdUlVnVFVt1TVd4HTgccnediAx0xjZqI1/z4OHJDkqCR7AT9Np6en2z/TaZwOBH4S+I9JTgSoqnOAfwTe1vzhngH8QlXtWf1CSbYCHwT+O7AFOBq4rEdMDwL+FHgUsBX4F+APmuf4fuBtwHOran/g6V3P8dvAh+n0Uh3evE4vTwQur6rqWnZ5s1zS+C1iuyOpXbY78Ezg61X1jQG315jt3XYAmoiVb3n+Dvg88LXulVW13HX38iRnAT8O/M9m2avoJCrLwP+qqg/0eZ2XA39bVWc197/R/NxP0wC8b+V+kt8BLura5F7gSUm+UlU3ADc0y79Lp7F6ZFVdB3ysTxwPBb65atk3gf37bC9p9Bat3ZHUvoVtd5qesrcDv7Letpoce7QWw58B/wE4mVXd6ABJnpbkoqZr+5vAK4H7JpRW1W3AXwBPAn5vjdc5ArhmvWCSfF+SP07y5SS3Ax8FDkyyV1X9M51voV4J3JDkr5L8cPPQX6UzBPCSJFck+fk+L3EncMCqZQcAd6wXm6SRWbR2R1L7FrLdaYY3fhj4w67kT1PARGsBVNWX6UwSfR7w/h6b/DmdyZRHVNUPAO+g8wcOdCr4AT8PnEWnm7ufrwKPGSCk19GZuPm0qjqATlc3K69ZVR+qqucAh9L5RuqdzfKvV9UvVtUjgV8C/jDJY3s8/xXAk5Oka9mTm+WSJmAB2x1JLVvEdifJQXSSrPOr6ncGiEkTZKK1OE4Bjmu+QVltf+CWqvp2kmPofBsEQJL96IxxfgPwc8BhSX65z2u8B3h2kpck2TvJw9K7rPr+dMYp35bkYOCNXa/3iCQvaMYu30Wnd+qeZt2LuyaR3grUyrpVlpvl/znJg5O8uln+kT5xSxqPRWp3SLJvE3uAfZLsl8T/s9JkLUy7k07hjg8Bf19Vp/Y7IGqP/wAWRFVdU1W7+6z+ZeC3ktwB/Dpwbte63wWuq6o/qqq7gJ8B3pTkyB6v8RU63yK9DriFzqTOp/R4vbfSKbl6M53Jq3/Tte5BzeOvb57jx5v4AH4UuDjJnXS+kXpNVX2xRxzfAU6kM077NjrfTp3YLJc0IYvU7jQ+TOek6unAzub2M/tsK2kMFqzdeWGz7c8lubPrZ2uf/deE5f6F2SRJkiRJw7JHS5IkSZJGzERLkiRJkkbMREuSJEmSRsxES5IkSZJGzERLkiRJkkZs70m+2CGHHFLbtm2b5EtKatGll156c1VtaTMG2x1pcdjm6P9v7/6jJLvL+86/P5aFwfyIJDPgsaRhyFomEGxEtpHJcuJtC4sVhEWQY1iULJbXOhmzx+SIhSwSZHfBwckRsY3sTVi8Y1AQNsuP8CPSIdhGESoTdrFgJAuBGBwBETBorBmMBJKdGAue/aNuS61RdXdV9a26t6rer3P6dNetW7ee6qr7nXn6+3yfK83bduPOXBOt/fv3c+jQVpc2kLRsknyl6xgcd6TV4Zgjad62G3csHZQkSZKklploSZIkSVLLTLQkSZIkqWUmWpIkSZLUsrGbYSQ5CTgEfL2qnp/kicB7gNOAm4CXVdV3ZhOmtpLBAIBaX+80Dkn9Nsjg/p/Xa72zOCRpXgaDALC+Xh1HolU1yYzWJcDhTbffBFxRVWcBdwEXtxmYJEmSJC2qsRKtJGcAfxd4W3M7wLnA+5tdrgJeOIsAJUmSJGnRjDuj9RvAa4DvNbd/CLi7qu5rbh8BTh/1wCQHkhxKcuj48eO7ClbS6khyUpI/TvLh5vYTk9yQ5LYk703ysK5jlCRJ2sqOiVaS5wPHqurGzZtH7DqyALaqDlbVWlWt7dnT6cXaJS0Wy5UlSdLCGmdG61nAC5LczrD5xbkMZ7hOSbLRTOMM4I6ZRChp5ViuLEmSFt2OiVZVvbaqzqiq/cBLgY9V1T8Argd+ttntIuDqmUUpadVMXa4MlixLkqTu7eY6WpcCr0ryRYb/CXp7OyFJWmW7LVcGS5YlSVL3xr6OFkBVDYBB8/OXgXPaD0nSitsoV34e8HDgMWwqV25mtSxXliRJvbabGS1Jap3lypIkaRmYaElaFJYrS5KkhTFR6aAkzZPlypIkaVE5oyVJkiRJLTPRkiRJkqSWWTooSZKkpTUYPHCFkPX1La8MIrXOCBoJLgAAIABJREFUGS1JkiRJapmJliRJkiS1zERLkiRJklrmGi1JkrTSkjwc+DjwAwz/b/T+qnp9kicC7wFOA24CXlZV3+kuUo1j85qsLrk2TM5oLaEMBmQw6DoMSZIWxV8C51bV04CzgfOTPBN4E3BFVZ0F3AVc3GGMkhaMiZYkSVppNXRvc/Pk5quAc4H3N9uvAl7YQXiSFpSJliRJWnlJTkpyM3AMuBb4EnB3Vd3X7HIEOL2r+CQtHhMtSZK08qrqu1V1NnAGcA7w5FG7jXpskgNJDiU5dPz48VmGKWmBmGhJkiQ1qupuYAA8EzglyUbjsDOAO7Z4zMGqWquqtT179swnUEm9Z6IlSZJWWpI9SU5pfn4E8DPAYeB64Geb3S4Cru4mQkmLyPbukiRp1e0FrkpyEsM/Qr+vqj6c5PPAe5L8CvDHwNu7DFKztdGO3VbsaouJliRJWmlVdQvw9BHbv8xwvZYkTczSQUmSJElqmYmWJEmSJLXM0kFJkiTpBBtrtsB1W5qOM1qSJEmS1LIdE60kD0/yqSSfSXJrkl9utj8xyQ1Jbkvy3iQPm324kiRJktR/48xo/SVwblU9DTgbOD/JM4E3AVdU1VnAXcDFswtTkiRJasdgkAeVBkqzsGOiVUP3NjdPbr4KOBd4f7P9KuCFM4lQ0spxJl2SJC26sZphNBfwuxH4UeAtwJeAu6vqvmaXI8DpM4lQ0iramEm/N8nJwCeS/B7wKoYz6e9J8lsMZ9Lf2mWgfTLI4CHb1mt97nFIkqQxm2FU1Xer6mzgDIYX7nvyqN1GPTbJgSSHkhw6fvz49JFKWhnOpEuSpEU3UdfBqrobGADPBE5JsjEjdgZwxxaPOVhVa1W1tmfPnt3EKmmFJDkpyc3AMeBanEmXJM3BOOu3XOOlcYzTdXBPklOanx8B/AxwGLge+Nlmt4uAq2cVpKTV40y6JElaZOPMaO0Frk9yC/Bp4Nqq+jBwKfCqJF8Efgh4++zClLSqnEmXJEmLaMdmGFV1C/D0Edu/zPCvzJLUqiR7gL+qqrs3zaS/iQdm0t+DM+mSJKnHxuo6KElzthe4qul4+n3A+6rqw0k+D7wnya8Af4wz6a3Y6FZoh0JJktpjoiUAMhjc/3Otr3cWhwTOpEuSpMU3UddBSZIkSdLOnNGSJElSb21uo76+PrLZ7FTHmuZxJz5/m7Fp+ZhoLajNpX7TPGajPHCa40iSJEnanqWDkiRJktQyZ7QkqcfsCChp1YxT3rdVKd+sTVt6OM4xLT1cPs5odSyDwY7le+Ps0+bjJEmSJO2OiZYkSZIktczSQUlaYhulh5Ikab5MtDqyVUnfLC4cPI/yQS94LEmStD3XY60WSwclSZIkqWXOaElSD41T8mdHQkmS+stEa4HMq4PgxvNYAihJkiRNx9JBSZK00pKcmeT6JIeT3Jrkkmb7aUmuTXJb8/3UrmOVtDic0ZKknphXh8CtSg43P/8k90lL4D7g1VV1U5JHAzcmuRb4eeC6qro8yWXAZcClHcYpaYE4oyVJklZaVR2tqpuan+8BDgOnAxcAVzW7XQW8sJsIJS0iZ7QkSZIaSfYDTwduAB5fVUdhmIwledwWjzkAHADYt2/ffAIV8EC7dLBluvrHREuSFoAXHpZmL8mjgA8Ar6yqbyfZ6SEAVNVB4CDA2tqa/9uXBFg6KEmSRJKTGSZZ76qqDzab70yyt7l/L3Csq/gkLR5ntLQrk7act3W8JKlvMpy6ejtwuKrevOmua4CLgMub71d3EJ6kBWWiJUkLrk9lhV5EWQvqWcDLgM8mubnZ9jqGCdb7klwMfBV4cUfxqbF5TdYk93VtnNg29nGt2fLYMdFKcibwTuCHge8BB6vqN5OcBrwX2A/cDrykqu6aXaiSJEntq6pPAFv9T/jZ84xF0vIYZ43WxrUlngw8E/ilJE9heC2J66rqLOC65ramlMFg4jK8vlqm1yJJkiRNY8dEy2tLSJq3JGcmuT7J4SS3Jrmk2X5akmuT3NZ8P7XrWDW9QQa9KnuUpDYMBul1GaPmZ6Kug9tdWwIYeW0JSZqCM+mSJGmhjd0MY9prS3gRv34ap7TPDoHqSvPHm40/5NyTZPNM+nqz21XAALi0gxAlSZK2NdaM1m6uLVFVB6tqrarW9uzZ00bMklbINDPpSQ4kOZTk0PHjx+cV6lLqqrzPskJJ0qIbp+ug15aQ1IlpZ9Kr6iBwEGBtbc0+uZLUc12taWrzeds6lm3el8c4pYNeW2ICm0vylqXkbtrXtNvSQ0sXV9t2M+lVdXS7mXRJkqSu7ZhoeW0JSfPmTLokSVp0YzfDkKQ5ciZdkiQtNBOtOZhHCZwXCNYycSZdkiQtOhMtSVpRbXX1G3WcjW3rtT7R48fZX5KkRTDRBYslSZIkSTtzRmuXJikLXIbyvmV4DZIkaXdsQb47XbWz13yZaEmS5soLEUuSVoGlg5IkSZLUMme0WmJJnSRJ0mQsodva5t+NJZqLyRktSZIkSWqZM1qSpIW2XXt4W8dLkrpiotVjliNKkiRJi8nSQUmSJLVmMMiWa6+2u09b8/e2mEy0JEmSJKllJlqSJEmS1DITLXUqg4Fr0SRJkrR0bIYhSR3Z6IhnN7wHbO4S2IfjSFKfbKzT8rpai8EZLUmSJElqmTNa6oTlgpIkSVpmJlqSNEejStr6WOY2TkyT7DNteeR2zzHqmPOIScspyZXA84FjVfXUZttpwHuB/cDtwEuq6q6uYuyj7UrZbEeuVWfpoCRJErwDOP+EbZcB11XVWcB1zW1JGoszWuq9UWWGtb4+9zgkScurqj6eZP8Jmy8A1pufrwIGwKVzC0rSQjPRkqQ56GN54LT68lr6EoeW2uOr6ihAVR1N8riuA5K0OHZMtKxZliRJ2lqSA8ABgH379nUczey45qqfTnxfbP3eH+Os0XoH1ixPxYvxDm38HvxdaFxJrkxyLMnnNm07Lcm1SW5rvp/aZYySVsKdSfYCNN+Pjdqpqg5W1VpVre3Zs2euAUrqrx0Trar6OPDNEzZfwLBWmeb7C1uOS9JqewcL/AeeQQb3f2nI34kW1DXARc3PFwFXdxiLpAUz7Rota5YlzYyL0iXNW5J3MxxjHpvkCPB64HLgfUkuBr4KvLi7CKUHWMa5GGbeDGNV6pa1O5vLCsfpKLixv90HV4p/4JE0M1V14RZ3PXuugUhaGtNeR2usmmWwblnS/CU5kORQkkPHjx/vOhwtAMsaJUltmzbRsmZZ0rz5Bx5JkrQwdky0mprlTwJPSnKkqVO+HDgvyW3Aec3tpWXHvPnq+vfd9fNrS/6BR5KkXRgM4vquOdpxjZY1y5LmzUXpkiRp0c28GYYkTco/8EiSpEW38onWdt3rtiofs6xsOfm+SpI0H5avzc7m3+36eu24z077anorn2hJ0izZya6/5vXebDzPeq3P5fkkSf0wbddBSZIkSdIWnNGagKVlkiRJksZhoiVJLbFMsN/afn82H2+jLLCtMsFRx5bmbWMdzzhrd1xztTxGrfGa5LOgB1g6KEmSJEktM9GSJEmSpJZZOqils13L/q32lca1XbmYFt84pX+zfL/9LEnS8jDRkiRJkpbQJGvnXGfXPksHJUmSJKllKzmjtV25mKVkq8H3WdI4xu3+d2LJ36gSQC9cLEmrZSUTLUmSJEnb266csK2W7/NoHT+qZf08WDooSZIkSS1zRqthKdnyGfWejtOJcLtjTft4LT67wa2W7Ur/5vV8kqTF5oyWJEmSJLXMGS1JkiTdzzbf2sqoz8aJa6xG7TPqvnmulerK0iRa45R2WR64fCZ9TyfZf9pjt1WeuPn5LVnsJ8u9NI1pPzd2LZSkxWLpoCRJkiS1bGlmtCRJkvSAE0u4RpVqjVMmaCmh5mG7ssJFbQHf20Rr2rIpu8OpLyz9kzQLo0oPN8oJJy0vPHF/yxMlqT2WDkqSJElSy0y0JEmSJKlluyodTHI+8JvAScDbquryVqI68Xmm6BZoh0HNwrTdLbf7PJ54rHE+u9uVJe72Qs2jjt2nktxZjjujyqbsLKiuTfMZHKe8cNzHt1VGuNuyxi7NdNzZZu3JdmujplnD4lorzdJ2n6/t2sJPcsxJ11F1/ZmfekYryUnAW4DnAk8BLkzylLYCk6QTOe5ImjfHHUnT2k3p4DnAF6vqy1X1HeA9wAXthCVJIznuSJo3xx1JU9lN6eDpwNc23T4C/OSJOyU5ABxobt6b5E928Zx99FjgG10HMSO+ti20ORE9zrG222fEfQ95bdPGe+LjpjjOE6Z86q3MZ9zZ3Rvcl/PGOB5sueOYbCB5aAwTDjK7sl0cu3/+tsccGGPcmXLMOeH1T/qL3mr/XpUH9uW8mzVfZ+t2+7+jqR/XvMaJjr3luLObRGtUBA8plqyqg8DBXTxPryU5VFVrXccxC762xbTMr40FGHf68vs3DuPocwx9imMMO44704w5C/T6p7YKrxF8ncuk7de4m9LBI8CZm26fAdyxu3AkaVuOO5LmzXFH0lR2k2h9GjgryROTPAx4KXBNO2FJ0kiOO5LmzXFH0lSmLh2sqvuSvAL4A4btTq+sqltbi2xxLG1ZJL62RbW0r21Bxp2+/P6N48GM4wF9iAH6E8e2ZjjuLMTr36VVeI3g61wmrb7GVO3cg16SJEmSNL7dlA5KkiRJkkYw0ZIkSZKklploTSHJlUmOJflc17G0LcmZSa5PcjjJrUku6TqmNiV5eJJPJflM8/p+ueuY2pbkpCR/nOTDXceySvoyLvThHO7bedaHcyLJ7Uk+m+TmJIc6jOOUJO9P8oXmM/K3O4jhSc3vYePr20leOe84upLk/CR/kuSLSS7rOp5Z6Mt4OGt9GG9nrW/j+ay1/e+Fa7SmkOSngHuBd1bVU7uOp01J9gJ7q+qmJI8GbgReWFWf7zi0ViQJ8MiqujfJycAngEuq6o86Dq01SV4FrAGPqarndx3PqujLuNCHc7hv51kfzokktwNrVdXpRU2TXAX8h6p6W9NB7wer6u4O4zkJ+Drwk1X1la7imJfm9f5H4DyGbeM/DVy4LP/GbujLeDhrfRhvZ61v4/mstf3vhTNaU6iqjwPf7DqOWaiqo1V1U/PzPcBh4PRuo2pPDd3b3Dy5+VqavzYkOQP4u8Dbuo5l1fRlXOjDOdyn88xz4gFJHgP8FPB2gKr6TpdJVuPZwJdWIclqnAN8saq+XFXfAd4DXNBxTK3ry3g4a30Yb2etT+P5rM3i3wsTLW0pyX7g6cAN3UbSrmZa+GbgGHBtVS3T6/sN4DXA97oORN3r8hzu0XnWl3OigI8muTHJgY5i+OvAceBfN6Uxb0vyyI5i2fBS4N0dxzBPpwNf23T7CEv2H/NVtaz/Z4Jejeez1vq/FyZaGinJo4APAK+sqm93HU+bquq7VXU2cAZwTpKlKGtI8nzgWFXd2HUs6l7X53AfzrOenRPPqqq/BTwX+KWmtGrevh/4W8Bbq+rpwJ8Dna0RakoXXwD8m65i6EBGbFvK2YFV0vV4O2t9GM9nbVb/Xpho6SGaGtwPAO+qqg92Hc+sNCUzA+D8jkNpy7OAFzRrQd4DnJvkd7sNSV3o0znc8XnWm3Oiqu5ovh8DPsSwhGzejgBHNv01+v0ME6+uPBe4qaru7DCGeTsCnLnp9hnAHR3Fohb0abydtSX8f9NmM/n3wkRLD9Isenw7cLiq3tx1PG1LsifJKc3PjwB+BvhCt1G1o6peW1VnVNV+huU4H6uq/7HjsDRnfTiH+3Ke9eWcSPLIZqE8Tanec4C5d2Orqj8FvpbkSc2mZwNdLtq/kNUqG4Rh84uzkjyxmdF7KXBNxzFpSn0Yb2etL+P5rM3q3wsTrSkkeTfwSeBJSY4kubjrmFr0LOBlDDP5jda7z+s6qBbtBa5PcgvDf/CurSrboGvXejQu9OEc9jx7sMcDn0jyGeBTwL+rqt/vKJZ/BLyreW/OBv55F0Ek+UGGnfeWegbgRFV1H/AK4A8YNk54X1Xd2m1U7evReDhrfRhvZ83xfBds7y5JkiRJLXNGS5IkSZJaZqIlSZIkSS0z0ZIkSZKklploSZIkSVLLTLQkSZIkqWUmWpIkSZLUMhMtSZIkSWqZiZYkSZIktcxES5IkSZJaZqIlSZIkSS0z0ZIkSZKklplo6X5JfivJ/97yMdeTHGnzmJKWh+OOpHlyzNE8mWitgCS3J/lOkseesP3mJJVkP0BVvbyq3thFjG1J8gNJrkzy7SR/muRVXcckraIVG3dekuT/S/IXSQZdxyOtohUbc34tyW1J7knyhSQ/13VMGs1Ea3X8J+DCjRtJfhx4RHfhzMwbgLOAJwA/DbwmyfmdRiStrlUZd74J/AZwedeBSCtuVcacPwf+e+CvARcBv5nkv+k2JI1iorU6fgfY/BePi4B3bt4hyTuS/Erz83qSI0leneRYkqNJ/qetDp7ktCT/OskdSe5K8m+32O+yJF9q/grz+SQv2nTfjyb5wyTfSvKNJO9ttifJFU0c30pyS5KnbhHKzwFvrKq7quow8NvAz+/865E0Aysx7lTVv6+q9wF3jPuLkTQTqzLmvL6qvlBV36uqG4D/APztMX9HmiMTrdXxR8Bjkjw5yUnA/wD87g6P+WGGfy05HbgYeEuSU7fY93eAHwT+JvA44Iot9vsS8Hea4/4y8LtJ9jb3vRH4KHAqcAbwL5vtzwF+Cvgx4JQm9j878cBNbD8CfGbT5s80MUmav6UfdyT1ysqNOUkeATwDuHWnfTV/JlqrZeMvPecBXwC+vsP+fwX806r6q6r6CHAv8KQTd2oGj+cCL29mkv6qqv5w1AGr6t9U1R3NX2HeC9wGnLPp+Z4A/EhV/Zeq+sSm7Y8G/gaQqjpcVUdHHP5Rzfdvbdr2reaxkrqx7OOOpH5ZtTHntxj+UfkPxthXc2aitVp+B/j7DEvp3rn9rgD8WVXdt+n2X/BAMrPZmcA3q+qunQ6Y5OcyXJh6d5K7gacCGwtXXwME+FSSW5P8AkBVfQz4V8BbgDuTHEzymBGHv7f5vvm+xwD37BSXpJlZ9nFHUr+szJiT5FebY7+kqmqnuDR/JlorpKq+wnCh6POAD7Z46K8BpyU5ZbudkjyB4ZqpVwA/VFWnAJ9jOOBQVX9aVf+wqn4E+EXg/0ryo819/2dV/dcMp+t/DPhfTzx+M/gdBZ62afPTcDpd6syyjzuS+mVVxpwkv8xwhu05VfXtXb86zYSJ1uq5GDi3qv68rQM2U9u/x3CwODXJyUl+asSujwQKOA7QLDi9f6FnkhcnOaO5eVez73eTPCPJTyY5mWGnnf8CfHeLcN4J/G9NHH8D+IfAO3b9IiXtxlKPO0lOSvJw4PuB70vy8OZxkrqx7GPOaxnO2p1XVa4d7TETrRVTVV+qqkMzOPTLGNYXfwE4BrxyxHN/Hvh14JPAncCPA//vpl2eAdyQ5F7gGuCSqvpPDMv/fpvhgPQVhotDf22LOF7PcBHqV4A/BH61qn5/ty9O0vRWYNx5GfCfgbcyXAD/n5vHSurACow5/xzYB9yW5N7m63W7f3lqWyzplCRJkqR2OaMlSZIkSS0z0ZIkSZKklploSZIkSVLLTLQkSZIkqWXfP88ne+xjH1v79++f51NK6tCNN974jara02UMjjvS6nDMkTRv2407c0209u/fz6FDs+i2KamPknyl6xgcd6TV4Zgjad62G3csHZQkSZKklploSZIkSVLLTLQkSZIkqWUmWpIkSZLUMhMtSZIkSWrZXLsOSosggwEAtb7eyeMlaVKDDABYr/VO45A0H4NB7v95fb06jETbcUZLUu8kOTPJ9UkOJ7k1ySXN9jck+XqSm5uv53UdqyRJ0ijOaEnqo/uAV1fVTUkeDdyY5Nrmviuq6tc6jE2SJGlHJlpSY6PkT92rqqPA0ebne5IcBk7vNipJkqTxWTooqdeS7AeeDtzQbHpFkluSXJnk1M4CkyRJ2oaJlqTeSvIo4APAK6vq28Bbgf8KOJvhjNevb/G4A0kOJTl0/PjxucWrxTLI4P6vto7VljZjkyR1w0RLUi8lOZlhkvWuqvogQFXdWVXfrarvAb8NnDPqsVV1sKrWqmptz5498wtakiSp4RotSb2TJMDbgcNV9eZN2/c267cAXgR8rov4JElaVraOb4+JlqQ+ehbwMuCzSW5utr0OuDDJ2UABtwO/2E14kiRJ2zPRktQ7VfUJICPu+si8Y5EkSZqGiZYkSRPYaFCxXuu9PebmJhptxilJGp/NMCRJkiSpZSZakiRJktQyEy1JkiRJatmOiVaShyf5VJLPJLk1yS8325+Y5IYktyV5b5KHzT5cqR0ZDMhg0HUYklaAFx6WpNU0zozWXwLnVtXTgLOB85M8E3gTcEVVnQXcBVw8uzAlSZIkaXHsmGjV0L3NzZObrwLOBd7fbL8KeOFMIpQkSZKkBTPWGq0kJzUXDT0GXAt8Cbi7qu5rdjkCnD6bECVJkiRpsYx1Ha2q+i5wdpJTgA8BTx6126jHJjkAHADYt2/flGFK8zdqDVetr2+5z4n3SZIkaXVN1HWwqu4GBsAzgVOSbCRqZwB3bPGYg1W1VlVre/bs2U2skiStrN021bAphyTN1zhdB/c0M1kkeQTwM8Bh4HrgZ5vdLgKunlWQkiRJs2KHZfXdYBAGg7R+vDaPqYcaZ0ZrL3B9kluATwPXVtWHgUuBVyX5IvBDwNtnF6YkSdLM2GFZUut2XKNVVbcATx+x/cvAObMIStqNjXVTXa+ZGmeNlySpe1VVwFYdlv9+s/0q4A3AW+cdn6TFNNEaLUmSpGVkh2VJbRur66AkSVpM4zTA2NhnvdZnGkuf2WFZfbGxbmp9feTHbapjbWjjmBqfiZZWyqhyvr7oS8mjJK2yqro7yYBNHZabWa1tOywDBwHW1tb8n6wkwNJBST2U5Mwk1yc53HQAu6TZflqSa5sOYNcmObXrWCUtPjssS5oFEy1JfXQf8OqqejLDvyr/UpKnAJcB1zUdwK5rbkvSbtlhWVLrLB2U1DtVdRQ42vx8T5LDDBehXwCsN7tdxfAC6pd2EKKkJWKHZUmzYKKlhXLiOqbNa65OXNs0zpqnPq/Z0lCS/Qz/A3QD8PgmCaOqjiZ53BaPcWG6JtJWM4jtjtN2w4lxmlzs9LhVbn4hSbNm6aCk3kryKOADwCur6tvjPq6qDlbVWlWt7dmzZ3YBSpIkbcEZLUm9lORkhknWu6rqg83mO5PsbWaz9jK83o0kSSvvxFbu6p4zWpJ6J0kYLjo/XFVv3nTXNQw7f4EdwCRJUo85o6Xe6+M6qj7GtGSeBbwM+GySm5ttrwMuB96X5GLgq8CLO4pPkiRpWyZaknqnqj4BbFUD8ex5xiJtZdpmFH19HkmLabclgxuPX1/3Wttts3RQkiRJklpmoiVJkiRJLTPRkiRJkqSWuUZLkiRJ6pl5t2u3PXz7TLQkSZqjjeYW67W+5X2SpMVnoqWl16dW7Bux1Pp6p3FIkiRptlyjJUmSJGlLg0EsLZyCiZYkSZIktcxES5IkSZJa5hotLaRR667msRZru+fo01owaZlN0kxi1D67fd6295UkLScTLUmSJGkFzGOd1ebnWF+vmT9fn1k6KEmSJEktM9GSJEmSpJaZaEmSJElSy0y0JEkLaZDBzJpOzPLYkqTVYKIlqXeSXJnkWJLPbdr2hiRfT3Jz8/W8LmOUJEnazo6JVpIzk1yf5HCSW5Nc0mw/Lcm1SW5rvp86+3Cl5ZDB4P4vjfQO4PwR26+oqrObr4/MOSZJkqSxjdPe/T7g1VV1U5JHAzcmuRb4eeC6qro8yWXAZcClswtV0qqoqo8n2d91HJIktWmj9fm0bc/n0Z593Ocf5zXs9vUuuh1ntKrqaFXd1Px8D3AYOB24ALiq2e0q4IWzClKSGq9IcktTWugsuiRJ6q2JLljc/IX56cANwOOr6igMk7Ekj2s9Okl6wFuBNwLVfP914BdG7ZjkAHAAYN++ffOKT1Pa3HRivdZ39fi2j71obOAhSf0xdjOMJI8CPgC8sqq+PcHjDiQ5lOTQ8ePHp4lRkqiqO6vqu1X1PeC3gXO22fdgVa1V1dqePXvmF6SkheR6dM3bYJD7v7S8xkq0kpzMMMl6V1V9sNl8Z5K9zf17gWOjHut/eCS1YWO8abwI+NxW+0rShDbWoz8ZeCbwS0mewnD9+XVVdRZwXXNbksYyTtfBAG8HDlfVmzfddQ1wUfPzRcDV7YcnaRUleTfwSeBJSY4kuRj4F0k+m+QW4KeB/6XTICUtDdejS5qFcdZoPQt4GfDZJDc3214HXA68r/kP0FeBF88mRGm1bLR8r/X1TuPoUlVdOGLz2+ceiKSV43p0SW3ZMdGqqk8AWxWQPrvdcCRJeqCpwywaWNgwYmer1kRkw4nr0YdFPWM9zgY8WspW5q4h252xm2FIkiQtK9ejS2qbiZYkSVpprkeXNAsTXUdLkiRpCbkeXZ1b1DK9RY17Hky0JEnSSnM9uqRZMNGSJPVG140q5vn80z5X178jSdJ4XKMlSZIkSS0z0VLvZDC4/1pSkiRJ0iIy0ZIkSZKklploSZIkSVLLbIYhSdIWbDwhSZqWM1qSJEmS1DITLUmSJElqmaWDkiRJUssGg62ugb16tvtdrK/XHCOZLxMt9daqt3jf/Pprfb2zOCRJkjQ5Ey1J0kQ2GkSs1/qDbm/e1vZzzeLYemizD5t/SFJ7XKMlqXeSXJnkWJLPbdp2WpJrk9zWfD+1yxglSZK244yWpD56B/CvgHdu2nYZcF1VXZ7ksub2pR3EJkkS0M46rEVdy7Wocc+TiZbmamPdkWuOdmfZ129V1ceT7D9h8wXAevPzVcAAEy1JktRTlg5KWhSPr6qjAM33x3UcjyRJ0pac0ZK0dJIcAA4A7Nu3r+NoVsuJjTIkadVYUqcNJlrq1Kq3cNdE7kxmyQNaAAAKfklEQVSyt6qOJtkLHNtqx6o6CBwEWFtbW94LdEiSpN6ydFDSorgGuKj5+SLg6g5jkSRJ2paJlqTeSfJu4JPAk5IcSXIxcDlwXpLbgPOa25IkSb1k6aCk3qmqC7e469lzDURTG2et1qQXx93txXS9GO/ujHpPXZOnVeU6rPZs/C7X15ev0t9ES51wbZYkSZKWmaWDkiRJktQyZ7QkSZK0kjaXAI5TumbJoCbhjJYkSZIktWzHGa0kVwLPB45V1VObbacB7wX2A7cDL6mqu2YXphaZ67G2N87vZ2OfWl+faSzSiXbb7MBmCZKkVTXOjNY7gPNP2HYZcF1VnQVc19yWJEmSJDFGolVVHwe+ecLmC4Crmp+vAl7YclySJElzk+TKJMeSfG7TttOSXJvktub7qV3GqPkYDPKQtVijtkk7mXaN1uOr6ihA8/1xW+2Y5ECSQ0kOHT9+fMqnkyRJmql3YAWPpBbNvBlGVR2sqrWqWtuzZ8+sn06SJGliVvBIatu07d3vTLK3qo4m2QscazMoSdLkNhpPwGTNJ0Y1rNh8rFG3pRXxoAqeJCMreJIcAA4A7Nu3b47hSeqzaWe0rgEuan6+CLi6nXAkSZIWi9U7kkYZp737u4F14LFJjgCvBy4H3pfkYuCrwItnGaTmq61W4rZ1b5+/U0maKyt4JE1tx0Srqi7c4q5ntxyLJElSn2xU8FyOFTySJjTzZhiSJEl911TwfBJ4UpIjTdXO5cB5SW4DzmtuS9JYpm2GIUlaUKOaX+zmOG3vq34b9V629ZnqkhU8ktrmjJYkSZIktcwZLUkLJcntwD3Ad4H7qmqt24gkSX0wGASA9fXqOBJNY+P9g+V5D020JC2in66qb3QdhCRJ0lYsHdSuZDCw5XhP+F5IkiT1hzNakhZNAR9NUsD/XVUHT9whyQHgAMC+ffvmHJ60XGxkIknTMdGStGieVVV3JHkccG2SL1TVxzfv0CRfBwHW1taWo9BbkrRrk6zj2rxmSJqGpYOSFkpV3dF8PwZ8CDin24gkSZIeyhktbWljvU+tr4+977j7qz3jrMua5L3ssySPBL6vqu5pfn4O8E87DkuSJOkhTLQkLZLHAx9KAsPx6/+pqt/vNiRJUh9N2u7dUkG1zURL0sKoqi8DT+s6jr7YaFKwXutj77vdNpseSJLUHtdoSZIkSVLLnNFaAaPW55y4bdzrL42z1sdrOXXL9XKSpGUwqvRv0nLAnY6tfhrnfW7rszBLzmhJkiRJUstMtCRJkiSpZZYOLqFJSvcmaQ2uxeN7143NTSXGaVTR5vNJfTFJsxZJWkYmWpIkSVparsdaXIu+VsvSQUmSJElqmYmWJEmSJLXM0sGeWZTW3K79kSRJ8yjb2q70b9R9lgpqQ9dlhSZakjSB7RpdjNOUYrt9xmkaMGnjCxtlaJ5Gfd5siiFpVVk6KEmSJEktc0arxzbK87YrIRxnH2nDqJLPcT5f4+wrSZKkB5hoSZIkqTUnrosZtWbqxPsmXUPjOqzVMkmb9532m+bY07J0UJIkSZJa5oyWJDV226hi2mNPsk+bzyfN06imGON8Tm2iIWlR7SrRSnI+8JvAScDbquryVqKi3Tbn81jHtN1zjLpvkvbo46yrGed4tmTXdhbl8zHLcUeSRpnluLPb0rlpSqQ2G1Xet1XJ3+bnGqd0b5LyPksBNY5pPyfjlBVOW3q4nalLB5OcBLwFeC7wFODCJE9pJSpJGsFxR9K8Oe5ImtZu1midA3yxqr5cVd8B3gNc0E5YkjSS446keXPckTSV3SRapwNf23T7SLNNkmbFcUfSvDnuSJrKbtZojSqSfEhBY5IDwIHm5r1J/qSNJwIeC3yjheO0arvn2HTfxLFP8jwztuvYO2TsI0zyWZric/eEyR8ycQgzGXfGeubRv5AHv1f9WXbQ189/H+PqY0ywLHFNek5Mtn/bY85WETxo3GlnzJl2sNjtIPOgxzfv5VbHHPN/OZM/7yz09XyZlq9nIrv9rE70+dxy3NlNonUEOHPT7TOAO07cqaoOAgd38TwjJTlUVWttH3cejL0bxr4UOh13xtHX98q4xtfHmMC4OrTjuNPlmNOmZXovl+m1gK9nUe2mdPDTwFlJnpjkYcBLgWvaCUuSRnLckTRvjjuSpjL1jFZV3ZfkFcAfMGx3emVV3dpaZJJ0AscdSfPmuCNpWru6jlZVfQT4SEuxTGqRp+iNvRvGvgQ6HnfG0df3yrjG18eYwLg6swDjTluW6b1cptcCvp6FlKp2LsglSZIkSRrazRotSZIkSdIIC51oJXlDkq8nubn5el7XMU0qyT9OUkke23Us40ryxiS3NL/zjyb5ka5jGleSX03yhSb+DyU5peuYxpXkxUluTfK9JEvfqWfRJflHSf6kec/+RdfxQL/HzL6NhX0d5/o4hjk2LZ++nY/T6ut5PK0+nv/TWpVxY6ETrcYVVXV287VQ9dNJzgTOA77adSwT+tWq+omqOhv4MPB/dB3QBK4FnlpVPwH8R+C1Hcczic8Bfw/4eNeBaHtJfhq4APiJqvqbwK91HNJmvRszezoW9nWc6+MY5ti0RHp6Pk6rr+fxtPp4/k9rJcaNZUi0FtkVwGsYccHVPquqb2+6+UgWKP6q+mhV3dfc/COG10NZCFV1uKp2f+FdzcP/DFxeVX8JUFXHOo6n73o3FvZ1nOvjGObYtHR6dz5Oq6/n8bT6eP5Pa1XGjWVItF7RTKFemeTUroMZV5IXAF+vqs90Hcs0kvyzJF8D/gGL+xeiXwB+r+sgtJR+DPg7SW5I8odJntF1QJv0aszs81i4AOOcY5ha1efzcVoLcB5Py/N/AfS+62CSfw/88Ii7/gnDbP4bDP9C8UZgb1X9whzD29YOsb8OeE5VfSvJ7cBaVX1jnvFtZ7vYq+rqTfu9Fnh4Vb1+bsHtYJzYk/wTYA34e9Wjk2DM2AfAP66qQ/OMTQ+2w/n9z4CPAZcAzwDeC/z1eXzW+jhm9nUs7Os418cxzLFpefT1fJxWX8/jafXx/J+W48YCJFrjSrIf+HBVPbXjUHaU5MeB64C/aDadAdwBnFNVf9pZYFNI8gTg3y3C731DkouAlwPPrqq/2Gn/vln2QWkZJPl9hqWDg+b2l4BnVtXxTgPbpA9j5qKMhX0b5/o6hjk2LbZFOR+n1bfzeFp9Pf+ntezjxq4uWNy1JHur6mhz80UMF9b1XlV9Fnjcxu1F+avRhiRnVdVtzc0XAF/oMp5JJDkfuBT4b5dhgFJv/VvgXGCQ5MeAhzGcSepU38bMPo+FfR3nHMM0K30+H6fV1/N4Wp7/i2ehZ7SS/A5wNsMymNuBX9z0n4iFsWiDWZIPAE8Cvgd8BXh5VX2926jGk+SLwA8Af9Zs+qOqenmHIY0tyYuAfwnsAe4Gbq6q/67bqDRKkocBVzIcn77D8K91H+s2qv6PmX0aC/s6zvVxDHNsWk59Oh+n1dfzeFp9PP+ntSrjxkInWpIkSZLUR8vQdVCSJEmSesVES5IkSZJaZqIlSZIkSS0z0ZIkSZKklploSZIkSVLLTLQkSZIkqWUmWpIkSZLUMhMtSZIkSWrZ/w+TVu17rnay8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x720 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean0, mean1, mean2 = desc_cls0.loc['mean', :], desc_cls1.loc['mean', :], desc_cls2.loc['mean', :]\n",
    "std0, std1, std2 = desc_cls0.loc['std', :], desc_cls1.loc['std', :], desc_cls2.loc['std', :]\n",
    "max0, max1, max2 = desc_cls0.loc['max', :], desc_cls1.loc['max', :], desc_cls2.loc['max', :]\n",
    "min0, min1, min2 = desc_cls0.loc['min', :], desc_cls1.loc['min', :], desc_cls2.loc['min', :]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=4)\n",
    "axs[0][0].hist(mean0, bins=100, color='c')\n",
    "axs[0][0].set_title('Mean class 0')\n",
    "axs[0][1].hist(mean1, bins=100, color='m')\n",
    "axs[0][1].set_title('Mean class 1')\n",
    "axs[0][2].hist(mean2, bins=100, color='y')\n",
    "axs[0][2].set_title('Mean class 2')\n",
    "\n",
    "axs[1][0].hist(std0, bins=100, color='c')\n",
    "axs[1][0].set_title('Std class 0')\n",
    "axs[1][1].hist(std1, bins=100, color='m')\n",
    "axs[1][1].set_title('Std class 1')\n",
    "axs[1][2].hist(std2, bins=100, color='y')\n",
    "axs[1][2].set_title('Std class 2')\n",
    "\n",
    "axs[2][0].hist(max0, bins=100, color='c')\n",
    "axs[2][0].set_title('Max class 0')\n",
    "axs[2][1].hist(max1, bins=100, color='m')\n",
    "axs[2][1].set_title('Max class 1')\n",
    "axs[2][2].hist(max2, bins=100, color='y')\n",
    "axs[2][2].set_title('Max class 2')\n",
    "\n",
    "axs[3][0].hist(min0, bins=100, color='c')\n",
    "axs[3][0].set_title('Min class 0')\n",
    "axs[3][1].hist(min1, bins=100, color='m')\n",
    "axs[3][1].set_title('Min class 1')\n",
    "axs[3][2].hist(min2, bins=100, color='y')\n",
    "axs[3][2].set_title('Min class 2')\n",
    "\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qox6ryyzwdr"
   },
   "source": [
    "### Clean, split and normalize the data\n",
    "\n",
    "The raw data has no apperent issues. It looks like we don't have any cleaning to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef42jTuxEjnj"
   },
   "outputs": [],
   "source": [
    "cleaned_df = raw_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSNgdQFFFQ6u"
   },
   "source": [
    "Split the dataset into train, validation, and test sets. The validation set is used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. This is especially important with imbalanced datasets where [overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting) is a significant concern from the lack of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfxhKg7Yr1-b"
   },
   "outputs": [],
   "source": [
    "all_labels = np.array(cleaned_df.loc[:, 'y'])\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(cleaned_df, stratify=all_labels, test_size=0.15)\n",
    "train_labels = np.array(train_df.loc[:, 'y'])\n",
    "train_df, val_df = train_test_split(train_df, stratify=train_labels, test_size=0.15)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('y'))\n",
    "val_labels = np.array(val_df.pop('y'))\n",
    "test_labels = np.array(test_df.pop('y'))\n",
    "\n",
    "# get features\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "# form boolean arrays for plotting\n",
    "cls0_bool = train_labels == 0\n",
    "cls1_bool = train_labels == 1\n",
    "cls2_bool = train_labels == 2\n",
    "\n",
    "# one-hot encode labels for multiclass model\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3, dtype='int')\n",
    "val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=3, dtype='int')\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=3, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a_Z_kBmr7Oh"
   },
   "source": [
    "Normalize the input features using the sklearn StandardScaler.\n",
    "This will set the mean to 0 and standard deviation to 1.\n",
    "\n",
    "Note: The `StandardScaler` is only fit using the `train_features` to be sure the model is not peeking at the validation or test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO-qEUmJ5JQg"
   },
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "#train_features = np.clip(train_features, -5, 5)\n",
    "#val_features = np.clip(val_features, -5, 5)\n",
    "#test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XF2nNfWKJ33w"
   },
   "source": [
    "Caution: If you want to deploy a model, it's critical that you preserve the preprocessing calculations. The easiest way to implement them as layers, and attach them to your model before export.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ7m9nqDC3W6"
   },
   "source": [
    "### Look at the data distribution\n",
    "\n",
    "Next compare the distributions of the classes over a few features. Good questions to ask yourself at this point are:\n",
    "\n",
    "* Do these distributions make sense? \n",
    "    * Yes. You've normalized the input and these are mostly concentrated in the `+/- 2` range.\n",
    "* Can you see the difference between the distributions?\n",
    "    * Yes the bigger class contains a much higher rate of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls0_df = pd.DataFrame(train_features[cls0_bool], columns = train_df.columns)\n",
    "cls1_df = pd.DataFrame(train_features[cls1_bool], columns = train_df.columns)\n",
    "cls2_df = pd.DataFrame(train_features[cls2_bool], columns = train_df.columns)\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].scatter(cls0_df['x11'], cls0_df['x101'])\n",
    "axs[0].set_title('Class 0')\n",
    "axs[1].scatter(cls1_df['x11'], cls1_df['x101'])\n",
    "axs[1].set_title('Class 1')\n",
    "axs[2].scatter(cls2_df['x11'], cls2_df['x101'])\n",
    "axs[2].set_title('Class 2')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='x11', ylabel='x101')\n",
    "    \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFK1u4JX16D8"
   },
   "source": [
    "## Define the model and metrics\n",
    "\n",
    "Define a function that creates a simple neural network with a densly connected hidden layer, a [dropout](https://developers.google.com/machine-learning/glossary/#dropout_regularization) layer to reduce overfitting, and an output softmax layer (\"[This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.](https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JQDzUqT3UYG"
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "]\n",
    "\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential([\n",
    "          keras.layers.Dense(\n",
    "              16, activation='relu',\n",
    "              input_shape=(train_features.shape[-1],)),\n",
    "          keras.layers.Dropout(0.5),\n",
    "          keras.layers.Dense(3, activation='softmax')])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=metrics,\n",
    "        weighted_metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU0GX6E6mieP"
   },
   "source": [
    "### Understanding useful metrics\n",
    "\n",
    "Notice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n",
    "\n",
    "\n",
    "\n",
    "*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n",
    "*   **True** negatives and **true** positives are samples that were **correctly** classified\n",
    "*   **Accuracy** is the percentage of examples correctly classified\n",
    ">   $\\frac{\\text{true samples}}{\\text{total samples}}$\n",
    "*   **Precision** is the percentage of **predicted** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n",
    "*   **Recall** is the percentage of **actual** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n",
    "*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n",
    "\n",
    "Note: Accuracy is not a helpful metric for this task. You can 99.8%+ accuracy on this task by predicting False all the time.  \n",
    "\n",
    "Read more:\n",
    "*  [True vs. False and Positive vs. Negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)\n",
    "*  [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "*   [Precision and Recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)\n",
    "*   [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYdhSAoaF_TK"
   },
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDbltVPg2m2q"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Now create and train your model using the function that was defined earlier. Notice that the model is fit using a larger than default batch size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no fraudulent transactions to learn from.\n",
    "\n",
    "\n",
    "Note: this model will not handle the class imbalance well. You will improve it later in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouUkwPcGQsy3"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xlR_dekzw7C"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wx7ND3_SqckO"
   },
   "source": [
    "Test run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LopSd-yQqO3a"
   },
   "outputs": [],
   "source": [
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKIgWqHms_03"
   },
   "source": [
    "### Optional: Set the correct initial bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "These initial guesses are not great. You know the dataset is imbalanced. Set the output layer's bias to reflect that  [See: setting bias for multiclass classification python tensorflow keras](https://stackoverflow.com/questions/60307239/setting-bias-for-multiclass-classification-python-tensorflow-keras).  This can help with initial convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the frequencies of the three classes as $ f_1 $, $ f_2 $, $ f_3 $ and the biases as $ b_1 $, $ b_2 $, $ b_3 $:\n",
    "$$ f_1 = e^{b_1} / (e^{b_1} + e^{b_2} + e^{b_3}) $$\n",
    "$$ f_2 = e^{b_2} / (e^{b_1} + e^{b_2} + e^{b_3}) $$\n",
    "$$ f_3 = e^{b_3} / (e^{b_1} + e^{b_2} + e^{b_3}) $$\n",
    "\n",
    "Setting $ (e^b_1 + e^b_2 + e^b_3) $ as $ x $:\n",
    "$$ e^{b_1} / x = f_1 $$\n",
    "$$ e^{b_2} / x = f_2 $$\n",
    "$$ e^{b_3} / x = f_3 $$\n",
    "\n",
    "In our case:\n",
    "$$ e^{b_1} / x = 0.125 $$\n",
    "$$ e^{b_2} / x = 0.75 $$\n",
    "$$ e^{b_3} / x = 0.125 $$\n",
    "\n",
    "That means:\n",
    "$$ e^{b_1} = 0.125 * x $$\n",
    "$$ e^{b_2} = 0.75 * x $$\n",
    "$$ e^{b_3} = 0.125 * x $$\n",
    "\n",
    "Setting $ x = 1 $:\n",
    "$$ log_e(b_1) = 0.125 $$\n",
    "$$ log_e(b_2) = 0.75 $$\n",
    "$$ log_e(b_3) = 0.125 $$\n",
    "\n",
    "And finally solving for $b$ to get our initial biases:\n",
    "$$ b_1 = log_e(0.125) \\approx -2.08 $$\n",
    "$$ b_2 = log_e(0.75) \\approx -0.29 $$\n",
    "$$ b_3 = log_e(0.125) \\approx -2.08 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias0 = -2.08\n",
    "initial_bias1 = -0.29\n",
    "initial_bias2 = -2.08\n",
    "initial_bias = np.array([initial_bias0, initial_bias1, initial_bias2])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50oyu1uss0i-"
   },
   "outputs": [],
   "source": [
    "model = make_model(output_bias = initial_bias)\n",
    "model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EJj9ixKVBMT"
   },
   "source": [
    "### Checkpoint the initial weights\n",
    "\n",
    "To make the various training runs more comparable, keep this initial model's weights in a checkpoint file, and load them into each model before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tSUm4yAVIif"
   },
   "outputs": [],
   "source": [
    "initial_weights = os.path.join(tempfile.mkdtemp(), 'initial_weights')\n",
    "model.save_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVXiLyqyZ8AX"
   },
   "source": [
    "### Confirm that the bias fix helps\n",
    "\n",
    "Before moving on, confirm quick that the careful bias initialization actually helped.\n",
    "\n",
    "Train the model for 20 epochs, with and without this careful initialization, and compare the losses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm4-4K5RZ63Q"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "model.layers[-1].bias.assign([0.0, 0.0, 0.0])\n",
    "zero_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8DsLXHQaSql"
   },
   "outputs": [],
   "source": [
    "model = make_model(output_bias = initial_bias)\n",
    "model.load_weights(initial_weights)\n",
    "careful_bias_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels), \n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3XsMBjhauFV"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "    # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "                   color=colors[n], label='Train '+label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "                color=colors[n], label='Val '+label,\n",
    "                linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxFaskm7beC7"
   },
   "outputs": [],
   "source": [
    "plot_loss(zero_bias_history, \"Zero Bias\", 0)\n",
    "plot_loss(careful_bias_history, \"Careful Bias\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKMioV0ddG3R"
   },
   "source": [
    "The above figure makes it clear: In terms of validation loss, on this problem, this careful initialization gives a clear advantage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsA_7SEntRaV"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZKAc8NCDnoR"
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "model.load_weights(initial_weights)\n",
    "baseline_history = model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSaDBYU9xtP6"
   },
   "source": [
    "### Check training history\n",
    "In this section, you will produce plots of your model's accuracy and loss on the training and validation set. These are useful to check for overfitting, which you can learn more about in this [tutorial](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).\n",
    "\n",
    "Additionally, you can produce these plots for any of the metrics you created above. False negatives are included as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTSkhT1jyGu6"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2,2,n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "        plt.plot(history.epoch, history.history['val_'+metric],\n",
    "                 color=colors[0], linestyle=\"--\", label='Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6LReDsqlZlk"
   },
   "outputs": [],
   "source": [
    "plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCa4iWo6WDKR"
   },
   "source": [
    "Note: That the validation curve generally performs better than the training curve. This is mainly caused by the fact that the dropout layer is not active when evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJC1booryouo"
   },
   "source": [
    "### Evaluate metrics\n",
    "\n",
    "You can use a [confusion matrix](https://developers.google.com/machine-learning/glossary/#confusion_matrix) to summarize the actual vs. predicted labels where the X axis is the predicted label and the Y axis is the actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNS796IJKrev"
   },
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVWBGfADwbWI"
   },
   "outputs": [],
   "source": [
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels.argmax(axis=1), predictions.argmax(axis=1))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    print('')\n",
    "    print('Class 0:')\n",
    "    print('True predictions = ', cm[0][0])\n",
    "    print('False positives = ', cm[1][0] + cm[2][0])\n",
    "    print('False negatives = ', cm[0][1] + cm[0][2])\n",
    "    print('')\n",
    "    print('Class 1:')\n",
    "    print('True predictions = ', cm[1][1])\n",
    "    print('False positives = ', cm[0][1] + cm[2][1])\n",
    "    print('False negatives = ', cm[1][0] + cm[1][2])\n",
    "    print('')\n",
    "    print('Class 2:')\n",
    "    print('True predictions = ', cm[2][2])\n",
    "    print('False positives = ', cm[0][2] + cm[1][2])\n",
    "    print('False negatives = ', cm[2][0] + cm[2][1])\n",
    "\n",
    "    print(np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOTjD5Z5Wp1U"
   },
   "source": [
    "Evaluate your model on the test dataset and display the results for the metrics you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poh_hZngt2_9"
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "    print(name, ': ', value)\n",
    "\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(test_predictions_baseline, -1)\n",
    "unhotted_test_labs = np.argmax(test_labels, -1)\n",
    "BMAC = balanced_accuracy_score(unhotted_test_labs, prediction)\n",
    "print('BMAC: %.3f' % BMAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyZtSr1v6L4t"
   },
   "source": [
    "If the model had predicted everything perfectly, this would be a [diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix) where values off the main diagonal, indicating incorrect predictions, would be zero. In this case the matrix shows that you have relatively few false positives. However, you would likely want to have even fewer false negatives despite the cost of increasing the number of false positives. This trade off may be preferable.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cveQoiMyGQCo"
   },
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePGp6GUE1WfH"
   },
   "source": [
    "### Calculate class weights\n",
    "\n",
    "Passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjGWErngGny7"
   },
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / cls0)*(total)/3.0 \n",
    "weight_for_1 = (1 / cls1)*(total)/3.0\n",
    "weight_for_2 = (1 / cls2)*(total)/3.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk1OOE2ZSHzy"
   },
   "source": [
    "### Train a model with class weights\n",
    "\n",
    "Now try re-training and evaluating the model with class weights to see how that affects the predictions.\n",
    "\n",
    "Note: Using `class_weights` changes the range of the loss. This may affect the stability of the training depending on the optimizer. Optimizers whose step size is dependent on the magnitude of the gradient, like `optimizers.SGD`, may fail. The optimizer used here, `optimizers.Adam`, is unaffected by the scaling change. Also note that because of the weighting, the total losses are not comparable between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ589fn8ST3x"
   },
   "outputs": [],
   "source": [
    "weighted_model = make_model()\n",
    "weighted_model.load_weights(initial_weights)\n",
    "weighted_history = weighted_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_features, val_labels),\n",
    "    verbose=0,\n",
    "    # The class weights go here\n",
    "    class_weight=class_weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0ynYRO0G3Lx"
   },
   "source": [
    "### Check training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBe9FMO5ucTC"
   },
   "outputs": [],
   "source": [
    "plot_metrics(weighted_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REy6WClTZIwQ"
   },
   "source": [
    "### Evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nifqscPGw-5w"
   },
   "outputs": [],
   "source": [
    "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owKL2vdMBJr6"
   },
   "outputs": [],
   "source": [
    "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
    "                                           batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(test_predictions_weighted, -1)\n",
    "unhotted_test_labs = np.argmax(test_labels, -1)\n",
    "BMAC = balanced_accuracy_score(unhotted_test_labs, prediction)\n",
    "print('BMAC: %.3f' % BMAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTh1rtDn8r4-"
   },
   "source": [
    "Here you can see that with class weights the accuracy and precision are lower because there are more false positives, but conversely the recall and AUC are higher because the model also found more true positives. Despite having lower accuracy, this model has higher recall (and identifies more fraudulent transactions). Of course, there is a cost to both types of error (you wouldn't want to bug users by flagging too many legitimate transactions as fraudulent, either). Carefully consider the trade-offs between these different types of errors for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ysRtr6xHnXP"
   },
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18VUHNc-UF5w"
   },
   "source": [
    "### Oversample the minority class\n",
    "\n",
    "A related approach would be to resample the dataset by oversampling the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHirNp6u7OWp"
   },
   "outputs": [],
   "source": [
    "cls0_features = train_features[cls0_bool]\n",
    "cls1_features = train_features[cls1_bool]\n",
    "cls2_features = train_features[cls2_bool]\n",
    "\n",
    "cls0_labels = train_labels[cls0_bool]\n",
    "cls1_labels = train_labels[cls1_bool]\n",
    "cls2_labels = train_labels[cls2_bool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYfJe2Kc-FAz"
   },
   "source": [
    "#### Using `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usyixaST8v5P"
   },
   "source": [
    "If you're using `tf.data` the easiest way to produce balanced examples is to start with creating three separate datasets for the three classes and then merge them. See [the tf.data guide](../../guide/data.ipynb) for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF4OZ-rI6xb6"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "def make_ds(features, labels):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels)) #.cache()\n",
    "    ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
    "    return ds\n",
    "\n",
    "cls0_ds = make_ds(cls0_features, cls0_labels)\n",
    "cls1_ds = make_ds(cls1_features, cls1_labels)\n",
    "cls2_ds = make_ds(cls2_features, cls2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNQUx-OA-oJc"
   },
   "source": [
    "Each dataset provides `(feature, label)` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llXc9rNH7Fbz"
   },
   "outputs": [],
   "source": [
    "for features, label in cls1_ds.take(1):\n",
    "    print(\"Features:\\n\", features.numpy()[0:25])\n",
    "    print()\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLEfjZO0-vbN"
   },
   "source": [
    "Merge the three together using `experimental.sample_from_datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7w9UQPT9wzE"
   },
   "outputs": [],
   "source": [
    "resampled_ds = tf.data.experimental.sample_from_datasets([cls0_ds, cls1_ds, cls2_ds], weights=[0.3, 0.3, 0.3])\n",
    "resampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWXARdTdAuQK"
   },
   "outputs": [],
   "source": [
    "for features, label in resampled_ds.take(1):\n",
    "    print(label.numpy()[1:10])\n",
    "    print(len(label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irgqf3YxAyN0"
   },
   "source": [
    "To use this dataset, you'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each negative example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xH-7K46AAxpq"
   },
   "outputs": [],
   "source": [
    "resampled_steps_per_epoch = np.ceil(3.0*cls1/BATCH_SIZE)\n",
    "resampled_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ1BvEpcBVHP"
   },
   "source": [
    "### Train on the oversampled data\n",
    "\n",
    "Now try training the model with the resampled data set instead of using class weights to see how these methods compare.\n",
    "\n",
    "Note: Because the data was balanced by replicating the two minority classes, the total dataset size is larger, and each epoch runs for more training steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soRQ89JYqd6b"
   },
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n",
    "resampled_model.load_weights(initial_weights)\n",
    "\n",
    "# Reset the bias to zero, since this dataset is balanced.\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0, 0, 0])\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
    "\n",
    "resampled_history = resampled_model.fit(\n",
    "    resampled_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=resampled_steps_per_epoch,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=val_ds,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avALvzUp3T_c"
   },
   "source": [
    "If the training process were considering the whole dataset on each gradient update, this oversampling would be basically identical to the class weighting.\n",
    "\n",
    "But when training the model batch-wise, as you did here, the oversampled data provides a smoother gradient signal: Instead of each positive example being shown in one batch with a large weight, they're shown in many different batches each time with a small weight. \n",
    "\n",
    "This smoother gradient signal makes it easier to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klHZ0HV76VC5"
   },
   "source": [
    "### Check training history\n",
    "\n",
    "Note that the distributions of metrics will be different here, because the training data has a totally different distribution from the validation and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoUGfr1vuivl"
   },
   "outputs": [],
   "source": [
    "plot_metrics(resampled_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PuH3A2vnwrh"
   },
   "source": [
    "### Re-train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFLxRL8eoDE5"
   },
   "source": [
    "Because training is easier on the balanced data, the above training procedure may overfit quickly. \n",
    "\n",
    "So break up the epochs to give the `callbacks.EarlyStopping` finer control over when to stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_yn9I26qAHU"
   },
   "outputs": [],
   "source": [
    "resampled_model = make_model()\n",
    "resampled_model.load_weights(initial_weights)\n",
    "\n",
    "# Reset the bias to zero, since this dataset is balanced.\n",
    "output_layer = resampled_model.layers[-1] \n",
    "output_layer.bias.assign([0, 0, 0])\n",
    "\n",
    "resampled_history = resampled_model.fit(\n",
    "    resampled_ds,\n",
    "    # These are not real epochs\n",
    "    steps_per_epoch = 20,\n",
    "    epochs=10*EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    "    validation_data=(val_ds),\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuJYKv0gpBK1"
   },
   "source": [
    "### Re-check training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMycrpJwn39w"
   },
   "outputs": [],
   "source": [
    "plot_metrics(resampled_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUuE5HOWZiwP"
   },
   "source": [
    "### Evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0fmHSgXxFdW"
   },
   "outputs": [],
   "source": [
    "train_predictions_resampled = resampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_resampled = resampled_model.predict(test_features, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO0mMOYUDWFk"
   },
   "outputs": [],
   "source": [
    "resampled_results = resampled_model.evaluate(test_features, test_labels,\n",
    "                                             batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(resampled_model.metrics_names, resampled_results):\n",
    "    print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BMAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(test_predictions_resampled, -1)\n",
    "unhotted_test_labs = np.argmax(test_labels, -1)\n",
    "BMAC = balanced_accuracy_score(unhotted_test_labs, prediction)\n",
    "print('BMAC: %.3f' % BMAC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentspace",
   "language": "python",
   "name": "latentspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
