For task02, we got pretty good results early on using a support vector machine classifier. We tried several different other classifiers but they performed much worse, so we left them aside for the moment. We then tried to build neural networks to fit the data. We spent a lot of time and tried balanced class weights and oversampling approaches with different neural network architectures. But the average recall just didn't get as good as with the support vector machine classifier. We tried the keras tuner to randomly search for good network architectures and we played around with different weights. We saw a steady increase in the BMAC but the model couldn't surpass a public score of 0.71. Finally we settled for the SVM. We performed a randomized cross validated search and then finetuned the parameters using cross validated grid search. After closing in on good parameters we also started to tune the class weights based on the confusion matrix output. Our final support vector machine was set up as follows: SVC(class_weight = {0: 3.18, 1: 0.44444444, 2: 3.025}, kernel = 'rbf', tol = 0.001, gamma = 0.057, degree = 6, decision_function_shape = 'ovo', C = 0.6225). With 5-fold cross validation we got a BMAC of 0.7002 (+/- 0.0275). On the public part of the test set we scored 0.723687894983.