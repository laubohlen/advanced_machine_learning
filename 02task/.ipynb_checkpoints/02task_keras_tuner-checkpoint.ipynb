{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import sklearn\n",
    "import kerastuner\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pR_SnbMArXr7"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('./data/X_train.csv')\n",
    "y_train = pd.read_csv('./data/y_train.csv')\n",
    "X_train['y'] = y_train['y']\n",
    "raw_df = X_train.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "df = raw_df.copy()\n",
    "all_labels = np.array(df.loc[:, 'y'])\n",
    "\n",
    "# split and shuffle dataset\n",
    "train_df, test_df = train_test_split(df, stratify=all_labels, random_state=seed, test_size=0.20)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "y_train = np.array(train_df.pop('y'))\n",
    "y_test = np.array(test_df.pop('y'))\n",
    "\n",
    "# get features\n",
    "X_train = np.array(train_df)\n",
    "X_test = np.array(test_df)\n",
    "\n",
    "# one-hot encode labels for multiclass model\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3, dtype='int')\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3840, 1000) (960, 1000)\n",
      "(3840, 3) (960, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:\n",
      "    Total: 4800\n",
      "       Class 0: 600 (12.50% of total)\n",
      "       Class 1: 3600 (75.00% of total)\n",
      "       Class 2: 600 (12.50% of total)\n"
     ]
    }
   ],
   "source": [
    "cls0, cls1, cls2 = np.bincount(raw_df['y'])\n",
    "total = cls0 + cls1 + cls2\n",
    "print('Samples:\\n    Total: {}\\n \\\n",
    "      Class 0: {} ({:.2f}% of total)\\n \\\n",
    "      Class 1: {} ({:.2f}% of total)\\n \\\n",
    "      Class 2: {} ({:.2f}% of total)'.format(total, cls0, 100*cls0/total, cls1, 100*cls1/total, cls2, 100*cls2/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 2.67\n",
      "Weight for class 1: 0.44\n",
      "Weight for class 2: 2.67\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / cls0)*(total)/3.0\n",
    "weight_for_1 = (1 / cls1)*(total)/3.0\n",
    "weight_for_2 = (1 / cls2)*(total)/3.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3JQDzUqT3UYG"
   },
   "outputs": [],
   "source": [
    "METRICS = [keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None),\n",
    "           keras.metrics.Precision(name='precision'),\n",
    "           keras.metrics.Recall(name='recall'),\n",
    "           keras.metrics.AUC(name='auc')]\n",
    "\n",
    "n_features = X_train.shape[-1]\n",
    "def make_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(hp.Int('input_units', min_value=4, max_value=128, step=4, default=64),\n",
    "                                 activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'],\n",
    "                                                      default='relu'),\n",
    "                                 input_shape=(n_features,)))\n",
    "    model.add(keras.layers.Dropout(hp.Float('dropout', 0, 0.5, step=0.1)))\n",
    "    for i in range(hp.Int('n_layers', 1, 4)):\n",
    "        model.add(keras.layers.Dense(hp.Int(f'dense_{i}_units', min_value=4, max_value=128, step=4, default=32),\n",
    "                                     activation=hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'],\n",
    "                                                      default='relu')))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4,\n",
    "                                                           max_value=1e-2, sampling='LOG', default=1e-3)),\n",
    "                  loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=METRICS,\n",
    "                  weighted_metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(make_model,\n",
    "                    objective=kerastuner.Objective('val_recall', direction='max'),\n",
    "                    max_trials=20,\n",
    "                    executions_per_trial=2,\n",
    "                    directory='log_dir',\n",
    "                    seed=seed)\n",
    "\n",
    "tuner.search(x=X_train,\n",
    "            y=y_train,\n",
    "            epochs=40,\n",
    "            batch_size=128,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=0,\n",
    "            class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_units': 120, 'dense_activation': 'relu', 'dropout': 0.1, 'n_layers': 3, 'dense_0_units': 60, 'learning_rate': 0.000164395039994333, 'dense_1_units': 40, 'dense_2_units': 116, 'dense_3_units': 48}\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 120)               120120    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                2440      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 116)               4756      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 351       \n",
      "=================================================================\n",
      "Total params: 134,927\n",
      "Trainable params: 134,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tuner.get_best_hyperparameters()[0].values)\n",
    "#print(tuner.results_summary())\n",
    "print(tuner.get_best_models()[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "y = np.array(df.pop('y'))\n",
    "X = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_recall', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00024: early stopping\n",
      "BMAC: 0.9023\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n",
      "BMAC: 0.9051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "BMAC: 0.9051\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00016: early stopping\n",
      "BMAC: 0.9301\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "BMAC: 0.9111\n",
      "CV complete.\n"
     ]
    }
   ],
   "source": [
    "# define kfold for training set\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "score = list()\n",
    "\n",
    "# loop through folds\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "\n",
    "    # split into training and validation\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # feature selection\n",
    "    #X_train, X_val, cor = select_features(X_train, y_train, X_val, mutual_info_regression, n_features)\n",
    "\n",
    "    # scale training, validation and testing set\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # oen-hot encode labels\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=3, dtype='int')\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, num_classes=3, dtype='int')\n",
    "\n",
    "    # train the model\n",
    "    model = tuner.get_best_models()[0]\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks = [early_stopping],\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0,\n",
    "        class_weight=class_weight)\n",
    "\n",
    "    # predict on test data\n",
    "    test_predictions = model.predict(X_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # calculate average recall \n",
    "    prediction = np.argmax(test_predictions, -1)\n",
    "    unhotted_test_labels = np.argmax(y_val, -1)\n",
    "    BMAC = balanced_accuracy_score(unhotted_test_labels, prediction)\n",
    "\n",
    "    # store bmac\n",
    "    score.append(BMAC)\n",
    "    print('BMAC: {:0.4f}'.format(BMAC))\n",
    "\n",
    "print('CV complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9107 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.4f (+/- %0.2f)\" % (np.mean(score), np.std(score) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.read_csv('./data/X_test.csv')\n",
    "X_new = X_new.drop(['id'], axis=1)\n",
    "X_new = scaler.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prediction = model.predict(X_new, batch_size=BATCH_SIZE)\n",
    "nn_prediction = np.argmax(nn_prediction, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = np.array(range(len(X_new)))\n",
    "df = pd.DataFrame({'id': ID,\n",
    "                    'y': nn_prediction})\n",
    "name = 'nn_test.csv'\n",
    "path = os.path.join('.', name)\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentspace",
   "language": "python",
   "name": "latentspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
